<!DOCTYPE html>
<html lang="en">
<head>
  
    <title>Why does my GAN do that? :: willsnell</title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="What are GANs? Much ink has already been spilled on the class of machine learning networks called GANs, or Generative Adversarial Networks, so I will only summarize it here.
If you&amp;rsquo;re interested in learning more, this short course is a great resource.
Although replaced in contemporary applications by diffusion models for tasks like image generation, GANs provide a unique opportunity to study the interplay of two tightly-coupled systems, each seeking a different goal." />
<meta name="keywords" content="" />

  <meta name="robots" content="noodp" />

<link rel="canonical" href="https://willsnell.com/posts/gans/" />


  






  
  
  
  
  
  <link rel="stylesheet" href="https://willsnell.com/styles.css">







  <link rel="shortcut icon" href="https://willsnell.com/img/theme-colors/orange.png">
  <link rel="apple-touch-icon" href="https://willsnell.com/img/theme-colors/orange.png">



<meta name="twitter:card" content="summary" />

  
    <meta name="twitter:site" content="" />
  
    <meta name="twitter:creator" content="" />



<meta property="og:locale" content="en" />
<meta property="og:type" content="article" />
<meta property="og:title" content="Why does my GAN do that?">
<meta property="og:description" content="What are GANs? Much ink has already been spilled on the class of machine learning networks called GANs, or Generative Adversarial Networks, so I will only summarize it here.
If you&amp;rsquo;re interested in learning more, this short course is a great resource.
Although replaced in contemporary applications by diffusion models for tasks like image generation, GANs provide a unique opportunity to study the interplay of two tightly-coupled systems, each seeking a different goal." />
<meta property="og:url" content="https://willsnell.com/posts/gans/" />
<meta property="og:site_name" content="willsnell" />

  
  
    
  
  <meta property="og:image" content="https://willsnell.com/posts/gans/creepy_face_upscaled.png">

<meta property="og:image:width" content="1200">
<meta property="og:image:height" content="627">


  <meta property="article:published_time" content="2024-07-22 09:25:17 &#43;1200 NZST" />












</head>
<body class="orange">


<div class="container center headings--one-size">

  <header class="header">
  <div class="header__inner">
    <div class="header__logo">
      <a href="/">
  <div class="logo">
    will_snell
  </div>
</a>

    </div>
    
      <ul class="menu menu--mobile">
  <li class="menu__trigger">Menu&nbsp;▾</li>
  <li>
    <ul class="menu__dropdown">
      
        
          <li><a href="/about">About</a></li>
        
      
      
    </ul>
  </li>
</ul>

    
    
  </div>
  
    <nav class="navigation-menu">
  <ul class="navigation-menu__inner menu--desktop">
    
      
        
          <li><a href="/about" >About</a></li>
        
      
      
    
  </ul>
</nav>

  
</header>


  <div class="content">
    
<article class="post">
  <h1 class="post-title">
    <a href="https://willsnell.com/posts/gans/">Why does my GAN do that?</a>
  </h1>
  <div class="post-meta"><time class="post-date">2024-07-22</time><span class="post-reading-time">12 min read (2360 words)</span></div>

  
  
  <img src="/posts/gans/creepy_face_upscaled.png"
    class="post-cover"
    alt="Why does my GAN do that?"
    title="Cover Image" />


  

  <div class="post-content"><div>
        <h2 id="what-are-gans">What are GANs?<a href="#what-are-gans" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>Much ink has already been spilled on the class of machine learning networks called
GANs, or Generative Adversarial Networks, so I will only summarize it here.</p>
<blockquote>
<p>If you&rsquo;re interested in learning more, <a href="https://developers.google.com/machine-learning/gan/gan_structure">this short course is a great resource.</a></p>
</blockquote>
<p>Although replaced in contemporary applications by diffusion models for
tasks like image generation, GANs provide a unique opportunity
to study the interplay of two tightly-coupled systems, each seeking a
different goal.</p>
<p>GANs consist of two distinct networks:</p>
<ul>
<li>a generator, which tries to generate new, convincingly realistic content, and</li>
<li>a discriminator, which tries to tell real content from fake.</li>
</ul>
<p>Generally, during training, the generator is allowed to back-propagate gradients
all the way through both the discriminator (as it assesses the generated images)
and the generator itself. Conversely, the discriminator has no special knowledge
of the inner workings of the generator.
Hence, if a generator&rsquo;s output fails to fool the discriminator, the
generator gets immediate feedback about what parts of the image tipped the discriminator off
to the fakery. But if a discriminator is repeatedly hoodwinked, it can only look inward to
understand how to improve.</p>
<p>Although GANs can technically generate any type of content, this post will focus on the generation
of images.</p>
<h2 id="visualizing-dcgans">Visualizing DCGANs<a href="#visualizing-dcgans" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>I particularly like using visualisation to get a deeper grip on a complex system
I am working with, and Deep Convolutional GANs (<a href="https://arxiv.org/pdf/1511.06434">as introduced in Radford, Metz &amp; Chintala</a>)
offered the perfect opportunity for some interesting visualisations.</p>

  <figure class="left" >
    <img src="dcgan_gen_architecture.png"   />
    
      <figcaption class="center" >The DCGAN architecture (Radford, Metz & Chintala), specifically the generator, with a 100-tall vector being processed through convolutional blocks that gradually decrease in feature depth and increase in resolution, until being projected to a 64 x 64 pixel RGB image.</figcaption>
    
  </figure>


<p>DCGANs use ordinary 2-dimensional convolutions in the discriminator to repeatedly downsample the image, building up
more and more internal features as the spatial resolution decreases.
In this way, they a variant of a typical convolutional image classifier.</p>
<blockquote>
<p>The architecture presented in the DCGAN paper omits residual/skip-connections, à la <a href="https://arxiv.org/pdf/1512.03385">ResNet</a>.
However, the two networks were introduced at similar times, and so the DCGAN&rsquo;s authors probably didn&rsquo;t shun them on purpose.</p>
</blockquote>
<p>The generator does effectively the same thing, but in reverse, taking a large (in this case 100-long)
vector of random noise, and up-projecting it via transposed convolutions to gradually increase the
image resolution while decreasing the depth of the feature dimension.</p>
<p>Because this network processes images, the very
first layer of the discriminator and the very last layer of the generator
should produce kernels that take in/out image data directly.</p>
<p>For a (4x4 kernel) convolutional
layer that takes in an RGB image and produces a 128-feature output, this comes out to:</p>
<ul>
<li>128 individual kernels (1 for each feature in the output layer),</li>
<li>of depth 3 (i.e. each kernel has different weights for how much it activates
on a red, green, or blue pixel)</li>
<li>of size 4 x 4</li>
</ul>
<p>Because each kernel has a depth of 3, we can visualize its weights using RGB images. In essence,
we get 128 x (4 high x 4 wide) images, where a bright red pixel indicates a part of the kernel
that activates strongly on red pixels, but not on green or blue, and a black pixel indicates no
activation for any input colour.</p>
<p>We can use this process in reverse to visualize the generator&rsquo;s <strong>output</strong> kernels, where a 128-feature
space is projected up into an RGB image by transpose convolutions (effectively, but not quite, a convolution in reverse.)
The generator&rsquo;s kernels are the different brushes with which the network paints the output image. Just like an artist might
need brushes of different sizes and shapes to effectively draw broad strokes and details, so the network might be expected
to need to specialize into different output kernels.</p>
<p>At least, that&rsquo;s my intuition for what would be expected to appear. In image classifiers (read: the discriminator),
the input kernels are often visualized in this way.</p>
<p>For example, this image shows (centre) the kernels of a ResNet as visualized by <a href="https://www.researchgate.net/publication/321192231">Jiang, et. al</a>:
<img alt="Resnet in Jiang, et. al" src="https://www.researchgate.net/publication/321192231/figure/fig4/AS:963217320841220@1606660313749/Visualization-of-first-layer-convolution-kernels-and-feature-maps-for-the-CS-ResCNN.gif"></p>
<h2 id="hypothesis-1">Hypothesis 1<a href="#hypothesis-1" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>Before running the experiment, I expected that the discriminator would
develop clearly identifiable features in its kernels, such as
alternating black/white lines in various orientations (useful for edge-detection,) potentially filters
of one main colour, etc.</p>
<p>I suspected the generator would be similar, but was less confident in this. After all, the kernel
visualizations I had seen, to date, were all of classifiers/discriminators, which serve a different
purpose to the generator I was going to train.</p>
<p>What I saw, however, was much more surprising.</p>
<h2 id="exploring-my-dcgan">Exploring my DCGAN<a href="#exploring-my-dcgan" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>I started with a DCGAN with feature layers of depth [128, 256, 512], kernel size (4 x 4), and which took in an 64x64 RGB image.
The generator had an input vector of size 100, and was trained on normally distributed random noise.</p>
<p>Training on the <a href="https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html">CelebA</a> dataset, I used a batch-size of 8, using PyTorch
with an Adam optimizer using default settings for both the generator and the discriminator.</p>
<p>The results from this first run are presented below.</p>
<blockquote>
<p>Note: I&rsquo;ve used a non-linear
time-step in the video since the model should change less and less as it gets further
and further through its training.</p>
</blockquote>
<video controls preload="auto" width="500px"  autoplay loop playsinline class="html-video">
    <source src="/posts/gans/gan_full.mp4" type="video/mp4">
  <span>Your browser doesn't support embedded videos, but don't worry, you can <a href="/posts/gans/gan_full.mp4">download it</a> and watch it with your favorite video player!</span>
</video>
<p><em>Training a [128, 256, 512] feature DCGAN on CelebA for 3 epochs.</em></p>
<p>These results were very surprising -
essentially the exact opposite of what I expected to see.
While I had suspected the generator kernels might be inscrutable, many of
them instead appeared to have an identifiable function.</p>
<p>On the other hand, the discriminator kernels appeared to not change at all
from their initialisation. Although I suspected an issue with the code,
further investigation showed the discriminator&rsquo;s kernels were, in fact, changing
throughout the run - just not enough to be discernible.</p>
<blockquote>
<p>The values in the input kernels to the discriminator do change, slightly.
In the visualization, the layer becomes noticeably less saturated from start to finish, even though
the absolute values of the layer weights do not change substantially. Since I normalize
each frame by the max and min pixel values, this suggests to me that a few pixels are
&lsquo;going hot&rsquo;, but that
they are not doing so in a recognisable pattern.</p>
</blockquote>
<h2 id="hypothesis-2">Hypothesis 2<a href="#hypothesis-2" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>My hypothesis, based on the above results, is as follows:</p>
<p>Assumptions:</p>
<ul>
<li>The capacity of a network to learn information during training should be correlated to
the size of the network.</li>
<li>A layer retaining its initial values, or something very close to them, suggests that
the exact distribution of these layers is unimportant to the network.</li>
</ul>
<p>Based on these (admittedly unproven) assumptions, I would make a few predictions:</p>
<ul>
<li>If a layer is unimportant to the network, the network is probably over-sized for the task it is being
trained on</li>
<li>Therefore, a smaller network (with its initial layer decreased in size) should be able to do equally well
on the task.</li>
<li>&ldquo;Doing equally well on the task&rdquo;, in this context, is not strictly what the discriminator is evaluated on (i.e. its ability
to distinguish real from fake images), but rather is the quality of images produced after the GAN is fully trained.</li>
<li>A well sized discriminator will show specialization in its input kernels.</li>
</ul>
<p>Boiling this down, I suspected that decreasing the number of input kernels of the discriminator (leaving the generator untouched)
would have little to no impact on the quality of images produced after training. I also suspected the combined network would
work equally well up to the point where the discriminator&rsquo;s input kernels showed strong specialization (i.e. something similar to
the generator&rsquo;s output kernels.)</p>
<h2 id="testing-hypothesis-2">Testing Hypothesis 2<a href="#testing-hypothesis-2" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>The obvious test was to decrease the number of input kernels for the discriminator
(i.e. the first-layer feature depth) from 128 to 64, and see what happens.</p>
<p>Even though GANs are notoriously unstable during training, this change didn&rsquo;t cause the network to diverge.
This suggests to me that the discriminator might have an
easier job to do than the generator, and so the smaller network was not immediately defeated
by its adversary.</p>
<video controls preload="auto" width="500px"  autoplay loop playsinline class="html-video">
    <source src="/posts/gans/first_layer_64.mp4" type="video/mp4">
  <span>Your browser doesn't support embedded videos, but don't worry, you can <a href="/posts/gans/first_layer_64.mp4">download it</a> and watch it with your favorite video player!</span>
</video>
<p>There are some interesting takeaways from these results:</p>
<ol>
<li>The output kernels from the generator look very similar to those from the previous
model. This suggests these kernels represent features useful to the generator,
regardless of the exact distribution of features in the hidden layers.</li>
<li>The discriminator&rsquo;s first layer still looks remarkably random. Despite being half as big,
the fully trained layer still looks like a desaturated version of the starting layer. At
least one recognisable feature seems to have appeared, though, in the 3rd column, 1st row.</li>
<li>The image quality produced by the fully trained generator looks noticeably worse than for
the previous network. Although broad facial features are still present, the images qualitatively
look desaturated and hazy.</li>
</ol>
<p>These findings suggest that although the input kernel&rsquo;s exact weights may not be important, it is important
for the discriminator to have access to lots of them. Even if the feature representation inside the model
is ultimately fed by a series of random convolutions, having that feature depth seems to give the discriminator
more tools with which to sniff out fraudulent images. Consequently, a smaller number of input kernels lets the
generator get away with worse images.</p>
<h2 id="putting-the-hypothesis-properly-to-bed">Putting the Hypothesis Properly to Bed<a href="#putting-the-hypothesis-properly-to-bed" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>To check that these findings hold, I ran 3 more models with smaller and smaller input kernels. In particular,
the number of input features to the discriminator were decreased to 32, 16, and then 8. Mainly, I wanted to see
if the discriminator would eventually need to start specializing these kernel weights, even if it didn&rsquo;t want to.</p>
<h3 id="32-input-kernels">32 Input Kernels<a href="#32-input-kernels" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<video controls preload="auto" width="500px"  autoplay loop playsinline class="html-video">
    <source src="/posts/gans/first_layer_32.mp4" type="video/mp4">
  <span>Your browser doesn't support embedded videos, but don't worry, you can <a href="/posts/gans/first_layer_32.mp4">download it</a> and watch it with your favorite video player!</span>
</video>
<p>The 32 input-kernel network follows the same trend as the 64 input-kernel model: increasingly hazy, desaturated images.</p>
<p>Two interesting notes, though:</p>
<ol>
<li>Even as the image quality is degraded, the quality of the facial features looks pretty similar.</li>
<li>More egregious structured artifacts are beginning to appear. In the lower left of this image, for example,
you can see a clear checker-boarding pattern in the background. Such patterns are commonly produced by
this network.</li>
</ol>
<p><img alt="Blocky, repeating artifacts visible in the lower-left-corner" src="/posts/gans/artefacts.png"></p>
<p>Together, these findings suggest to me that deeper layers of the discriminator
are still learning a good representation of facial features. However, the constrained
first layer has hindered the ability of the discriminator to notice and penalize noise and
overall &ldquo;image-quality&rdquo;.</p>
<p>I imagine each decrease in input kernels as the discriminator looking through an increasingly blurry
or perhaps tinted lens at the image, able to make out broad strokes but missing areas of high spatial frequency,
and seeing an increasingly desaturated version of the world.
The analogy isn&rsquo;t quite right, since the spatial dimensions the network receives are unchanged, but it&rsquo;s a start.</p>
<h3 id="the-edge-of-functionality---8-input-kernels">The Edge of Functionality - 8 Input Kernels<a href="#the-edge-of-functionality---8-input-kernels" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p>During training, the 16-kernel GAN diverged, meaning the last model left to explore has 8 input kernels.</p>
<video controls preload="auto" width="500px"  autoplay loop playsinline class="html-video">
    <source src="/posts/gans/first_layer_8.mp4" type="video/mp4">
  <span>Your browser doesn't support embedded videos, but don't worry, you can <a href="/posts/gans/first_layer_8.mp4">download it</a> and watch it with your favorite video player!</span>
</video>
<p>Finally, we see clear changes from the input kernels at the start to those at the end. However, this is
not enough to save the network, and the fully trained GAN produces images with little colour saturation,
degraded facial features, little separation between background and foreground, and most obviously, serious
checker-boarding artifacts across every part of the image.</p>
<p>It&rsquo;s perhaps remarkable that such a constrained discriminator could cause the generator to build up anything
resembling a human face, but the final results are indisputably bad, and much worse than previous models.</p>
<h2 id="conclusion">Conclusion<a href="#conclusion" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>Even though the input kernels to the discriminator in a DCGAN did not show much specialization during training,
reducing the number of input kernels available showed obvious reductions in image quality produced by the generator
after training. Further, the input kernels to the discriminator, no matter how few they were,
never showed the kind of specialization apparent in the final layers of the generator.</p>
<h3 id="other-wild-geese-to-chase">Other Wild Geese to Chase<a href="#other-wild-geese-to-chase" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p>Even though the DCGAN&rsquo;s Discriminator and Generator are largely symmetric, there are some key differences
that might explain the apparent lack of specialization in the discriminator&rsquo;s first layer.</p>
<ol>
<li>
<p>Activation functions: the generator uses Tanh between the transpose convolution and the final generated image,
while the discriminator a) uses LeakyReLU and b) places the activation function after the convolution. I haven&rsquo;t
visualized the effect of these activation functions (since doing so would require an input to be run through
the network, and the result would depend on said input.) It&rsquo;s quite possible the answer to this mystery lies
here.</p>
</li>
<li>
<p>The discriminator is not actually trained in the same way as typical classifiers. Although the architecture is
similar, the output of this discriminator is a single value (real or fake.)
It&rsquo;s possible random noise is perfectly sufficient for
this task, whereas specialization only becomes necessary for classifiers that need to distinguish between
multiple different classes of image.</p>
<p>Testing this would be as simple* as changing datasets (e.g. to <a href="https://github.com/zalandoresearch/fashion-mnist">fashion-MNIST</a>
or <a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10</a>), and changing the discriminator to
choose from 11 categories (the 10 from CIFAR-10, plus 1 extra denoting &ldquo;fake&rdquo;).</p>
<p><em>*No guarantees it will be this simple</em></p>
</li>
<li>
<p>The flow of gradients through the network for a GAN is asymmetric: the generator gets access
to the entire state of the discriminator for backprop, while the discriminator only gets its
own network&rsquo;s response to stimulus to learn from. Why this mismatch would lead to such a
difference in layer specialization is unknown, to me at least. Perhaps patterned specialization
in the discriminator&rsquo;s layer closest to the generator&rsquo;s own layers would provide an easy
attack vector for the generator to learn and defeat the discriminator?</p>
</li>
<li>
<p><em>Is the first layer actually random?</em> The lack of human-identifiable structure in the kernels
doesn&rsquo;t strictly mean the discriminator isn&rsquo;t specializing them. Freezing this layer and comparing
performance to the original network could tell whether the discriminator is, in fact, getting
value from this layer after all.</p>
</li>
</ol>

      </div></div>

  
    
<div class="pagination">
    <div class="pagination__title">
        <span class="pagination__title-h">Read other posts</span>
        <hr />
    </div>
    <div class="pagination__buttons">
        
        <span class="button previous">
            <a href="https://willsnell.com/posts/hyperspace/">
                <span class="button__icon">←</span>
                <span class="button__text">Navigating Hyperspace</span>
            </a>
        </span>
        
        
    </div>
</div>

  

  
    

  
</article>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright">
        <span>© 2024 Powered by <a href="https://gohugo.io">Hugo</a></span>
    
      <span>:: <a href="https://github.com/panr/hugo-theme-terminal" target="_blank">Theme</a> made by <a href="https://github.com/panr" target="_blank">panr</a></span>
      </div>
  </div>
</footer>






<script type="text/javascript" src="/bundle.min.js"></script>





  

  
    
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)']]                  
    }
  };
</script>


  
</div>

</body>
</html>
