<!DOCTYPE html>
<html lang="en">
<head>
  
    <title>Why Is Measuring Composition So Difficult? :: willsnell</title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content=" This post was inspired by, and heavily leans upon, the structure set out in &ldquo;A Mathematical Framework for Transformer Circuits.&rdquo; In it, the authors present a compelling way to decompose the transformer architecture into individual, more interpretable pieces. If you haven&rsquo;t read it yet, I&rsquo;d recommend doing so. Most of what I present won&rsquo;t make sense without context.
Motivation A transformer&rsquo;s ability to process long sequences of text is facilitated by multiple attention layers, which we can decompose into multiple attention heads per layer.
" />
<meta name="keywords" content=", " />

  <meta name="robots" content="noodp" />

<link rel="canonical" href="https://willsnell.com/posts/math_composition/" />






  
  
  
  
  
  <link rel="stylesheet" href="https://willsnell.com/styles.css">







  <link rel="shortcut icon" href="https://willsnell.com/img/theme-colors/orange.png">
  <link rel="apple-touch-icon" href="https://willsnell.com/img/theme-colors/orange.png">



<meta name="twitter:card" content="summary" />

  
    <meta name="twitter:site" content="" />
  
    <meta name="twitter:creator" content="Will Snell" />



<meta property="og:locale" content="en" />
<meta property="og:type" content="article" />
<meta property="og:title" content="Why Is Measuring Composition So Difficult?">
<meta property="og:description" content=" This post was inspired by, and heavily leans upon, the structure set out in &ldquo;A Mathematical Framework for Transformer Circuits.&rdquo; In it, the authors present a compelling way to decompose the transformer architecture into individual, more interpretable pieces. If you haven&rsquo;t read it yet, I&rsquo;d recommend doing so. Most of what I present won&rsquo;t make sense without context.
Motivation A transformer&rsquo;s ability to process long sequences of text is facilitated by multiple attention layers, which we can decompose into multiple attention heads per layer.
" />
<meta property="og:url" content="https://willsnell.com/posts/math_composition/" />
<meta property="og:site_name" content="willsnell" />

  
  
    
  
  <meta property="og:image" content="https://willsnell.com/posts/math_composition/DSC_0621.jpg">

<meta property="og:image:width" content="1200">
<meta property="og:image:height" content="627">


  <meta property="article:published_time" content="2024-10-12 19:57:00 &#43;1300 NZDT" />













  
    
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)']]                  
    }
  };
</script>


  

</head>
<body class="orange">


<div class="container center headings--one-size">

  <header class="header">
  <div class="header__inner">
    <div class="header__logo">
      <a href="/">
  <div class="logo">
    will_snell
  </div>
</a>

    </div>
    
      <ul class="menu menu--mobile">
  <li class="menu__trigger">Menu&nbsp;▾</li>
  <li>
    <ul class="menu__dropdown">
      
        
          <li><a href="/about">About</a></li>
        
      
      
    </ul>
  </li>
</ul>

    
    
  </div>
  
    <nav class="navigation-menu">
  <ul class="navigation-menu__inner menu--desktop">
    
      
        
          <li><a href="/about" >About</a></li>
        
      
      
    
  </ul>
</nav>

  
</header>


  <div class="content">
    
<article class="post">
  <h1 class="post-title">
    <a href="https://willsnell.com/posts/math_composition/">Why Is Measuring Composition So Difficult?</a>
  </h1>
  <div class="post-meta"><time class="post-date">2024-10-12</time><span class="post-author">Will Snell</span></div>

  
    <span class="post-tags">
      
      #<a href="https://willsnell.com/tags/"></a>&nbsp;
      
      #<a href="https://willsnell.com/tags/"></a>&nbsp;
      
    </span>
  
  
  <img src="/posts/math_composition/DSC_0621.jpg"
    class="post-cover"
    alt="Why Is Measuring Composition So Difficult?"
    title="Cover Image" />


  

  <div class="post-content"><div>
        <blockquote>
<p>This post was inspired by, and heavily leans upon, the structure set out in
<a href="https://transformer-circuits.pub/2021/framework/index.html">&ldquo;A Mathematical Framework for Transformer Circuits.&rdquo;</a>
In it, the authors present
a compelling way to decompose the transformer architecture into individual,
more interpretable pieces. <br>
<br>
If you haven&rsquo;t read it yet, I&rsquo;d recommend doing so. Most of what I present
won&rsquo;t make sense without context.</p>
</blockquote>
<h1 id="motivation">Motivation<a href="#motivation" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h1>
<p>A transformer&rsquo;s ability to process long sequences of text is
facilitated by multiple attention layers, which we can decompose into
multiple attention <em>heads</em> per layer.</p>
<p>Because all information moving between tokens in a sequence must pass through an attention head,
tracing how they are wired together should give us a circuit diagram of where a good chunk of
the internal information is moving, and might let us
decompose the monolithic transformer into many smaller, interconnected pieces.</p>
<p>While MLPs can and do move information, attention heads can <em>only</em> move and scale information,
and do so in an almost linear way.</p>
<p>We have a strong motivation to want to analyse a model by direct inspection of its weights,
rather than by actually using the model for inference. Firstly, running models can be computationally expensive, while
inspecting weights is typically quite cheap.
Secondly, if we have to run the model, we need data for it to process. If we want to make
strong conclusions about our observations, we need a breadth and depth of data, and often
we need to curate the data appropriately. Direct inspection of weights bypasses all of that - the
numbers that make up the model are unchanging (once trained) and easily accessible.</p>
<p>If we can treat attention head information movement as linear, we gain access to a host of linear
algebra tools to analyse the system. The promise of this approach is a large and reasonably comprehensive circuit diagram,
purely by inspecting the values of the models&rsquo; various weight matrices.</p>
<p>I am not the first to try and measure composition this way. In
<a href="https://transformer-circuits.pub/2021/framework/index.html">A Mathematical Framework</a>,
the authors use the Frobenius norm to measure how much two heads compose with one another.</p>
<p>In <a href="https://www.lesswrong.com/posts/vaHxk9jSfmGbzvztT/thoughts-on-formalizing-composition">Thoughts on Formalizing Composition</a>,
Lieberum takes this further, axiomatizing what it means to measure composition. These axioms can
be summarized (imprecisely) as follows:</p>
<h3 id="lieberums-axioms">Lieberum&rsquo;s Axioms<a href="#lieberums-axioms" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
\[B \in \mathbb{R}^{m \times n}, \;A \in \mathbb{R}^{n \times m}\]<p>\(A\) is the matrix in an early head that <strong>writes to</strong> the residual stream.</p>
<p>\(B\) is the matrix in a later head that <strong>reads from</strong> the residual stream.</p>
\[C(A,B)\]<p>\(C \) takes the two matrices and outputs some kind of a score.</p>
<h4 id="axiom-1">Axiom 1:<a href="#axiom-1" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h4>
<p>If \(C(A,B) = 1\) - that is, two heads are composing as much as possible - \(A\) should only
write where \(B\) can read.</p>
<h4 id="axiom-2">Axiom 2:<a href="#axiom-2" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h4>
<p>If \(C(A,B) = 0\) - the two heads are not composing at all - \(B\) cannot read anything that
\(A\) writes.</p>
<h4 id="axiom-3">Axiom 3:<a href="#axiom-3" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h4>
<p>\(C\) should vary &lsquo;smoothly&rsquo; and &lsquo;monotonically&rsquo; in some sense, i.e. if there is more overlap, then \(C\)
should be larger, and vice versa.</p>
<p>Axiom 3 is fairly complicated, and really says 2 different things:</p>
<ol>
<li>\(C\) should be smooth: We cannot just count the number of overlapping dimensions
between our writing and reading heads. Instead, there now exists a grey area where
a writing dimension partially overlaps a reading dimension, and our metric must account
for this. In some sense, we can describe this as how much the reading head is &ldquo;focused&rdquo;
on the writing head, versus just randomly reading in some of its outputs as background noise.</li>
<li>\(C\) should be monotonic: More overlap should lead to a higher \(C\) score.</li>
</ol>
<p>We can extend this definition to make it more rigorous, in the spirit of what Axiom 3 was trying
to convey. We will then show that, assuming communication in the residual stream is linear (i.e. ignoring input norm and MLPs),
none of the current methods for analysing composition meet axioms 3 and 4.</p>
<h4 id="proposed-axiom-4">(Proposed) Axiom 4:<a href="#proposed-axiom-4" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h4>
<p>\(C\) should measure the underlying information flow, and thus the relative ordering of composition scores between
pairs of heads should be preserved when the model is changed in a way that does not affect information flow.</p>
\[
\begin{align}
C(A^*, B^*) > C(A^*, D^*) &\iff C(A, B) > C(A, D) \\
\\
    \text{Where } A^* &= T\:A \\
    B^* &= T^{-1}\:B \\
    D^* &= T^{-1}\:D \\
\end{align}
\]<p>Where \(T\) is an invertible matrix which exploits a symmetry of the model to leave information flow unchanged.</p>
<p>A concrete example of \(T\) which will effectively always work is</p>
\[
T = \begin{bmatrix}
        0& 1& ... &0\\
        1& 0& ... &0\\
        \vdots & \vdots & \vdots & \vdots\\
        0 &... &...  & 1\\
\end{bmatrix}
\]<p>That is, we simply swap the order of the first and second residual stream directions, and leave everything
else unchanged. Heads which previously wrote to direction 1 will now write to direction 2, and vice versa.
A head which previously read from direction 1 will now read from direction 2, etc. The actual information
flow has not changed, and we have effectively just relabelled the axes.</p>
<p>All our typical tools for measuring composition easily survive this relabelling, and so they should!
We will see through the remainder of this post that there are subtler manipulations we can use which
pose actual problems if we hope to meet axioms 3 and 4.</p>
<h1 id="linear-analysis">Linear Analysis<a href="#linear-analysis" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h1>
<p>Following the work in <a href="https://transformer-circuits.pub/2021/framework/index.html">[1]</a> and
<a href="https://www.lesswrong.com/posts/vaHxk9jSfmGbzvztT/thoughts-on-formalizing-composition">[2]</a>,
we will begin by ignoring MLP layers and layer normalization. We know that both of these
non-linearities can contribute to information movement, and hence composition. Later on,
we&rsquo;ll touch on layer normalization.</p>
<p>For now, removing them gives us purely linear communication from component to component
in the residual stream.</p>
<h2 id="the-unavoidable-symmetries-of-the-residual-stream">The unavoidable symmetries of the residual stream.<a href="#the-unavoidable-symmetries-of-the-residual-stream" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<h4 id="symmetries-inside-heads">Symmetries <em>Inside</em> Heads<a href="#symmetries-inside-heads" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h4>
<p>&ldquo;A Mathematical Framework&rdquo; points out that the specific
vector we get when looking inside a transformer head is not
particularly meaningful. We&rsquo;ll work through the OV-circuit
to show why this is the case.</p>
<p>Denoting the residual stream a given head can read from as \(\vec{x}_0\), the residual stream the same
head writes to as \(\vec{x}_1\), and
the hidden layer inside the head itself as \(\vec{y}\), the OV circuit is implemented as:</p>
\[
\begin{align}
    \vec{x}_1 &= W_O\:\vec{y} \\
    \vec{y}   &= W_V\:\vec{x}_0
\end{align}
\]<p>We can make \(\vec{y}\) disappear by combining the O and V matrices, without any
loss of generality:</p>
\[
\begin{align}
    \vec{x}_1 &= (W_O W_V) \vec{x}_0 \\
              &= W_{OV} \vec{x}_0 \\
\end{align}
\]<p>It&rsquo;s common to think of the combination of the \(W_O\) and \(W_V\) matrices
as a single low-rank matrix \(W_{OV}\), exactly because we have so many
different ways to slice up \(W_O\) and \(W_V\), all of which are valid.</p>
<p>To be precise, for any invertible matrix of appropriate shape \(T\):</p>
\[
\begin{align}
    \vec{x}_1 &= W_O\:(T\:T^{-1})\:\vec{y} \\
              &= (W_O\:T)(T^{-1}\:W_V)\:\vec{x}_0 \\
              &= (W_O^*)(W_V^*)\:\vec{x}_0 \\
    \vec{y}^* &= W_V^*\:\vec{x}_0
\end{align}
\]<p>We can pick the basis of \(\vec{y}^*\) by any (invertible) transformation.
The dimensionality of \(\vec{y}^*\) cannot be changed (because then \(T\) wouldn&rsquo;t be invertible),
but otherwise we are free to do whatever we want.</p>
<p>More than that, we should <strong>expect</strong> any applicable \(T\) to have <strong>already been applied</strong> by the time
we are handed our model to analyse.</p>
<p>Our model and any techniques we apply must respect
that \(\vec{y}\) is <strong>symmetric</strong> with respect to the
<a href="https://en.wikipedia.org/wiki/General_linear_group">group \[\text{GL}_n(\mathbb{R})\]</a></p>
<p>In other words, there are an infinite collection of \(W_O\)&rsquo;s, and for every one a partner \(W_V\).
However, there is only one unique \(W_{OV}\).</p>
<p>The same analysis can be applied to the matrix used to compute the dot-product attention, \(W_Q^T\:W_K\).</p>
<h4 id="symmetries-between-heads">Symmetries <em>Between</em> Heads<a href="#symmetries-between-heads" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h4>
<p>If we&rsquo;re serious about ignoring LayerNorm (or its equivalent), we can extend this idea of symmetry
outwards, to the residual stream (\(\vec{x}\)) itself.</p>
<p>Looking at the first few layers, for a single attention head:</p>
\[
\begin{align}
            \vec{x}_0 &= W_E \\
            \vec{x}_n &= \mathbb{A} \cdot W_O W_V \vec{x}_{n-1} \\
            \mathbb{A} &= f(\vec{x}_{n-1}^T W_Q^T W_K \vec{x}_{n-1}) \\
            \ell &= W_U \vec{x}_{n}
\end{align}
\]<p>Where \(W_E\) is the embedding matrix, \(\mathbb{A}\) is the attention pattern calculated
between sequence positions, and \(\ell\) the log-probabilities.</p>
<p>(For simplicity, this notation hides the fact that the attention mechanism moves information
between the residual streams of <em>different tokens</em>, and so we really have <em>many</em> residual
streams. For the analysis we&rsquo;re doing here, this adds complexity but doesn&rsquo;t provide extra insight.)</p>
<p>We can multiply anything which writes to the residual stream by an invertible matrix \(T\),
and anything which reads from the residual stream
by its inverse \(T^{-1}\).
The following system is identical to the previous:</p>
\[
\begin{align}
            T \vec{x}_0 &= T\: W_E \\
\\
            T \vec{x}_n &= \mathbb{A} \cdot T\;W_O W_V\;T^{-1}\;T\vec{x}_{n-1} \\ 
                        &= \mathbb{A} \cdot (T W_O W_V T^{-1})\;T\vec{x}_{n-1} \\ 
\\
            \mathbb{A} &= f((T \vec{x}_{n-1})^T (T^{-1})^T W_Q^T W_K T^{-1}\;T \vec{x}_{n-1}) \\
                       &= f((T \vec{x}_{n-1})^T (T^{-T} W_Q^T W_K T^{-1})\;T \vec{x}_{n-1}) \\
\\
            \ell &= W_U\;T^{-1}\;T\;\vec{x}_{n}\\
\end{align}
\]<p>In other words, there are an infinite family of symmetric models. These are, for all intents and
purposes, <strong>the same model</strong>, but have completely different weights.</p>
<p>Just as \(W_O\) and \(W_V\) were not unique in our previous example, the aggregate matrices
\(W_O W_V\) and \(W_Q^T W_K\) are symmetric, and can be arbitrarily modified by our choice of \(T\).
The residual stream is symmetric about the group \(\text{GL}_n\), and (ignoring non-linearities), our
analysis must respect that.</p>
<p>It&rsquo;s worth pointing out that \(W_{OV}\) and \(W_Q^T\:W_K\) transform differently.
\(W_{OV}\) transforms by \(T (.) T^{-1}\), while \(W_Q^T\:W_K\) transforms by \(T^{-T}\:(.)\:T^{-1}\).</p>
<p>The composite matrix \(W_{Q}^TW_K\:W_{OV}\) therefore transforms by \(T^{-T}\:(.)\:T^{-1}\).</p>
<h4 id="a-concrete-example">A Concrete Example<a href="#a-concrete-example" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h4>
<p>To make real the concepts presented above, let&rsquo;s examine the
V-composition scores between one upstream head and two downstream heads, denoted \(W_{OV}^{up}\),
\(W_{OV}^{down, 1}\) and \(W_{OV}^{down, 2}\).</p>
<p>We&rsquo;ll describe these matrices using their compact <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">Singular Value Decomposition</a>.
That is, they are of the form</p>
\[W_{OV} = U \Sigma V^T\]<p>Where \(U\) is an <em>m x r</em> rectangular matrix describing where in the residual stream the matrix reads from, \(V\) is an
<em>n x r</em> matrix which describes where in the residual stream is written to, and \(\Sigma\) describes how the
vectors that are read in are scaled, before being written out. <em>m</em> is the dimension of the residual stream,
and <em>r</em> is the rank of the matrix.</p>
<p>Let&rsquo;s imagine we have a 3-dimensional residual stream, and our \(W_{OV}\) matrices are rank 1 <em>(m=3, r=1)</em>.</p>
<p>Lets imagine our matrices are:</p>
\[
\begin{align}
    W_{OV}^\text{up} = \begin{bmatrix}
                    0 & 5 & 0 \\
                    0 & 0 & 0 \\
                    0 & 0 & 0 \\
                  \end{bmatrix}
                =
                  \begin{bmatrix} 
                        1 \\
                        0 \\
                        0 \\
                  \end{bmatrix}
                  \begin{bmatrix}
                        5 \\
                  \end{bmatrix}
                  \begin{bmatrix}
                        0 & 1 & 0 \\
                  \end{bmatrix}
\end{align}
\]<p>The <em>3 x 3</em> matrix in the middle is the expanded form, and the 3 matrices on the right are
\(U\), \(\Sigma\), and \(V^T\), respectively.</p>
<p>\(U\) tells us this matrix reads exclusively from the first dimension of the residual stream,
\(\Sigma\) tells us it multiplies what it reads by 5, and \(V\) tells us it writes the result
to the second dimension of the residual stream.</p>
<p>For our remaining two matrices, we&rsquo;ll just look at the SVD form:</p>
\[
\begin{align}
    W_{OV}^\text{down,1} = \begin{bmatrix} 
                        0 \\
                        \frac{1}{\sqrt{2}} \\
                        \frac{1}{\sqrt{2}} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                        1 \\
                  \end{bmatrix}
                  \begin{bmatrix}
                        0 & 0 & 1 \\
                  \end{bmatrix}
\end{align}
\]\[
\begin{align}
    W_{OV}^\text{down,2} = \begin{bmatrix} 
                        \frac{1}{\sqrt{2}} \\
                        \frac{1}{\sqrt{2}} \\
                        0 \\
                  \end{bmatrix}
                  \begin{bmatrix}
                        1 \\
                  \end{bmatrix}
                  \begin{bmatrix}
                        0 & 0 & 1 \\
                  \end{bmatrix}
\end{align}
\]<p>Right now, we might expect the composition scores of our two downstream heads to be the same. Why?</p>
<ul>
<li>The angle between their reading vectors and the vector \(W_{OV}^\text{up}\) writes to
are the same.</li>
<li>They both scale what they read by the same amount</li>
<li>They both write to the same dimension</li>
</ul>
<p>If we pick a composition measure such as proposed by <a href="https://docs.google.com/presentation/d/1sDvhes-36GjUJxHtmya33alggx0gA0o0kOnC8NK6rZE/edit#slide=id.g132679aefe9_0_596">Turner &amp; Strand</a>,
we should see that we do, indeed, get identical scores:</p>
\[
    C(A, B) = \frac{\vert \vert BA \vert \vert_F }{\vert \vert \sigma^B \odot \sigma^A \vert \vert_2}
\]<p>Where \(\vert \vert . \vert \vert_F\) is the Frobenius norm, \(\sigma^A\) and \(\sigma^B\) are the
singular values of A and B, and \(\odot\) is the elementwise product.</p>
\[
\begin{align}
C(W_{OV}^\text{down,1}, W_{OV}^\text{up}) &\approx 0.707 \\
C(W_{OV}^\text{down,2}, W_{OV}^\text{up}) &\approx 0.707 \\
\end{align}
\]<p>However, we can manipulate these composition scores by
warping the residual stream. Let&rsquo;s choose a transformation matrix</p>
\[ T = \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0 \\
        0 & 0 & 10 \\
        \end{bmatrix}
\]<p>Likewise,</p>
\[ T^{-1} = \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0 \\
        0 & 0 & 0.1 \\
        \end{bmatrix}
\]<p>That is, we increase the magnitude of direction 3 by 10x, and leave directions 1 and 2 unchanged.
By our transformation rules from above, \(W_{OV} \rightarrow T W_{OV} T^{-1} \).</p>
<p>Note: we have <strong>not</strong> modified anything about the information flow (as explained in the previous section).
However, our new scores are:</p>
\[
\begin{align}
C(W_{OV}^\text{down,1}, W_{OV}^\text{up}) &\approx 0.0995 \\
C(W_{OV}^\text{down,2}, W_{OV}^\text{up}) &\approx 0.707 \\
\end{align}
\]<p>If we wanted to make \(C(W_{OV}^\text{down,1}, W_{OV}^\text{up})\) higher than
\(C(W_{OV}^\text{down,2}, W_{OV}^\text{up})\), we could scale <strong>down</strong> direction 3
by 10x (i.e. swap the definition of \(T\) and \(T^{-1}\)) and we&rsquo;d get the opposite
effect.</p>
<p>In other words, we can swap the order of our two composition scores at will. We can also
make the scores for a particular set of heads very close to 0, or very close to 1. There
are some caveats:</p>
<ul>
<li>We need the downstream matrices to read from slightly different directions
to one another, lest we be unable to separate them</li>
<li>We need the composition score to not start at 0 or 1</li>
</ul>
<p>For real matrices in real models, these conditions are effectively always met.</p>
<h4 id="why-does-this-matter">Why does this matter?<a href="#why-does-this-matter" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h4>
<p>\(\text{GL}_N(\mathbb{R})\) is a very extensive group. Not only does it contain the group
of all rotations and reflections \(\text{O}(n)\), but it allows us to arbitrarily scale
different directions of the residual stream, or shear different directions to bring them
arbitrarily close together (but never parallel), or far apart.</p>
<p>If we continue to work in this symmetric basis, we are left with very few linear algebra tools
to measure composition
that could not be confounded by an adversarial choice of \(T\). To enumerate:</p>
<ul>
<li>We can&rsquo;t rely on the angle between vectors (e.g. writing directions vs reading directions)
to be meaningful (unless they are exactly parallel or perpendicular), because a small angle could
just be the result of an adversarial \(T\) shearing directions to be close together.</li>
<li>We can&rsquo;t rely on the ordering of singular values (or eigenvalues). A singular value could be large
merely because an adversarial \(T\) scaled down the direction in the residual stream we
are reading from, or scaled up the direction we are writing to.</li>
</ul>
<h4 id="a-way-out">A Way Out<a href="#a-way-out" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h4>
<p>Possible ways to get out of this symmetry are:</p>
<ol>
<li>Observe the model during runtime, collecting statistics on the residual stream directions.
Use this data to rescale the residual stream to something more regular (assuming such a regularization
exists).</li>
<li>Gather statistics of the weight matrices of the model, which exploit the fact
that the same \(T\) and \(T^{-1}\) must be applied to all heads in the model at the same time,
and use these to normalize our measures of composition.</li>
<li>Perform the measurements in a way that removes the symmetry.</li>
</ol>
<p>Option 2 is actually fairly easy to do without leaving our linear world. This uses the same technique
that gave us the \(W_{OV}\) combined matrix, namely: climb out of the hidden layer where the symmetry exists.</p>
<p>Instead of analysing \(W_{OV}\) and \(W_{QK}\) within the residual stream,
we analyse them in the token space. That is, when performing measurements,
instead of considering \(W_{OV}\) we consider</p>
\[
        W_U W_O W_V W_E = W_U\:T^{-1}\:T\:W_O W_V\:T^{-1}\:T\:W_E \\
\]<p>and instead of \(W_Q^T\:W_K\), we look at</p>
\[
        W_E^T W_Q^T W_K W_E = W_E^T\:(T\:T^{-1})^T W_Q^T\;W_K\:T^{-1}\:T\:W_E \\
\]<p>This is a lot more unwieldy, given we&rsquo;ve gone from a ~1,000 dimensional space to 50,000+.
But, it does mean that all our linear analysis tools start working again.</p>
<p>Operating in the token space works because \(W_E\) only ever receives one-hot vectors,
and \(W_U\) must output into the token space for decoding, and so neither are symmetric
(beyond global scaling, which can be easily ignored.)</p>
<h1 id="what-about-normalization">What about Normalization?<a href="#what-about-normalization" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h1>
<p>Our linear model is well and good, but real transformers have normalization between reading the residual stream
and feeding it into the Q, K, and V matrices. Adding normalization to our simplified transformer:</p>
\[
\begin{align}
            \vec{x}_0 &= W_E \\
            \vec{x}_n &= \mathbb{A} \cdot W_O W_V\;\mathcal{G}(\vec{x}_{n-1}) \\
            \mathbb{A} &= f(\mathcal{G}(\vec{x}_{n-1})^T W_Q^T W_K\:\mathcal{G}(\vec{x}_{n-1})) \\
            \ell &= W_U\;\mathcal{G}(\vec{x}_{n})
\end{align}
\]<p>Where \(\mathcal{G}\) is a non-linear normalization function.</p>
<p>For our purposes, we&rsquo;ll consider <a href="https://arxiv.org/pdf/1910.07467">RMS norm</a>, used
in models like LLaMA and Gemma. This has the functional form:</p>
\[
\begin{align}
            \mathcal{G}(\vec{x}) = \frac{\vec{x}}{\text{RMS}(\vec{x})} \\
            \\
            \text{where RMS}(\vec{x}) = \sqrt{ \frac{1}{n} \sum_{i=1}^{n} x_i^2} \\ 
\end{align}
\]<p>Importantly for us, although the residual stream is still symmetric with this transformation,
it is only symmetric with respect to a matrix \(R\) that preserves the relation:</p>
\[
\begin{align}
            R^{-1}\:\mathcal{G}(R\:\vec{x}) &= \mathcal{G}(\vec{x})\\
            \therefore \mathcal{G}(R\:\vec{x}) &= R \mathcal{G}(\vec{x})\\
\end{align}
\]<p>\(\text{RMS}(\vec{x})\) is just \(\frac{1}{\sqrt{n}} \cdot \vert \vert \vec{x} \vert \vert_2\), and
so we can satisfy this relation with any matrix \(R\) that scales all distances from the origin by the
same value, \(\alpha\). This gives us the group \(\mathcal{O}(n)\) in combination with any scalar \(\alpha\) applied
globally.</p>
<p>Because \(\mathcal{O}(n)\) comprises all the reflections and rotations about the origin, it preserves not only
angles between vectors, but also the magnitude of singular values. In some sense, RMS norm leaves us only with
the most boring symmetries, implying <strong>all</strong> of our typical tools for measuring composition should be valid.</p>
\[
            W_U\:W_{OV}\:W_E \neq W_U\:W_{OV}\:\mathcal{G}(W_E)\\
\]<h2 id="things-get-murkier">Things Get Murkier<a href="#things-get-murkier" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>Real models use input normalizations, and so we should be able to conclude that the residual stream in a real
model is unique, modulo a rotation, reflection, or global scaling. Hence, any metric which is
invariant under these simple symmetries (the inner product, for one, or the relative sizes of singular values)
can be used to measure interactions in the residual stream.</p>
<p><strong>However</strong>, things are unfortunately not so simple. There is no clear consensus on what normalization does
at inference time, nor on whether its impact is meaningful:</p>
<ul>
<li>
<p><a href="https://arxiv.org/pdf/2409.13710">Heimersheim</a> removed LayerNorm entirely from GPT-2. After fine-tuning,
the resulting model performed almost as well as the original.</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/1911.07013">Xu et. al</a> find that LayerNorm provides most of its benefits by regularizing
<strong>gradients</strong> during <strong>backpropagation</strong>. They trained and evaluated models with LayerNorm, without LayerNorm, and with DetachNorm,
which behaves like LayerNorm but does not allow the gradients of the mean and variance terms to back-propagate.
DetachNorm performs the worst of the three, suggesting LayerNorm&rsquo;s benefit does not originate from
its effect at inference time.</p>
</li>
<li>
<p><a href="https://aclanthology.org/2023.findings-acl.895.pdf">Brody, Alon, &amp; Yahav</a> argue the opposite - that LayerNorm
is crucial to the ability of attention heads to learn and express certain functions easily. A major caveat is that
they studied very small models (8-dimensional residual stream, with 2-dimensional hidden head layers).</p>
<p>As models grow in size, they suggest the benefits of LayerNorm become less useful.</p>
</li>
</ul>
<p>If we really can ignore input normalizations and treat the residual stream as linear, then we are left with a
highly symmetric residual stream and need to use a technique that breaks the symmetry (such as working in token space.)
Actually removing the normalization functions with fine-tuning (as <a href="https://arxiv.org/pdf/2409.13710">Heimersheim did</a>)
is not ideal, because it requires considerable compute and leaves us analysing a different model than what we started with.</p>
<p>If input normalization <em>is</em> important to the expressivity and performance of a transformer, there still remains the question
of whether the residual stream is a meaningful-enough basis to analyse it in. As the dimensionality of the residual stream grows,
the impact of
any one direction on the normalization statistics (such as variance) shrinks. It&rsquo;s not hard to imagine scaling up
or down residual stream directions, perhaps enough to reorder the singular values in a few attention heads, without
meaningfully changing the effect of input normalization. We are no longer talking about <strong>identical</strong> models related by
a perfect symmetry, and so our conclusions are not quite so strong.</p>
<p>Nonetheless, if we work purely in the residual stream,
there still appears room for inner-products between vectors, or singular values of matrices, to be nudged
such that any score relying on these two metrics could be altered without meaningfully altering how the involved heads communicate
with one another.</p>
<p>In the formalism of axiom 4, it&rsquo;s hard to prove that two very similar (but not strictly identical) models will always
preserve \(C(A, B) > C(A, D) \therefore C(A^*, B^*) > C(A^*, D^*)\).</p>
<p>Finally, there&rsquo;s the (reasonably likely) case where the non-linearity offered by input normalization
is used by the model to do something useful, but which is orthogonal to interpretability. Said another way:
a model can transmit (effectively) the same information with a range of transformations \(T\),
but might learn a \(T\) to exploit non-linearities in the model for some other reason. If our composition scores
are sensitive to this \(T\), we can&rsquo;t assume we are measuring composition - and only composition - when we use linear algebra
to compute \(C(A, B)\).</p>
<h1 id="whats-next">What&rsquo;s next?<a href="#whats-next" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h1>
<p>I&rsquo;m fairly convinced at this point that trying to measure composition directly in the residual stream, using
linear algebra tools, is not as straightforward as hoped.
That&rsquo;s not to say, however, that we have no avenues left to pursue. Three options
available are:</p>
<ol>
<li>Analyse composition in the token basis. This lets us confidently work with angles between vectors,
and with relative ordering of (if not the absolute value of) singular values. However, this requires
once again ignoring the effects of input normalization, which I am not yet convinced is completely valid.</li>
<li>Find some way of normalizing the weight matrices, without resorting to actual model inference, in order
to bolster the linear algebra tools enough that they pass axiom 4.</li>
<li>Use some kind of inference-time methods that pass data through the network to measure composition. This gives up
a lot of our linear algebra tools, but lets us actually characterise what non-linear parts of the system
(the input normalizations and especially the MLPs) are doing.</li>
</ol>
<p>I&rsquo;m currently planning on how to pursue #3 - in particular, I suspect we have a lot of useful information via
the compute graphs and gradient functions embedded in the models, and intended for network training.</p>

      </div></div>

  
    
<div class="pagination">
    <div class="pagination__title">
        <span class="pagination__title-h"></span>
        <hr />
    </div>
    <div class="pagination__buttons">
        
        <span class="button previous">
            <a href="https://willsnell.com/posts/entropy/">
                <span class="button__icon">←</span>
                <span class="button__text">Entropy and Information Theory</span>
            </a>
        </span>
        
        
        <span class="button next">
            <a href="https://willsnell.com/posts/hyperspace/">
                <span class="button__text">Navigating Hyperspace</span>
                <span class="button__icon">→</span>
            </a>
        </span>
        
    </div>
</div>

  

  
    

  
</article>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright">
        <span>© 2024 Powered by <a href="https://gohugo.io">Hugo</a></span>
    
      <span>:: <a href="https://github.com/panr/hugo-theme-terminal" target="_blank">Theme</a> made by <a href="https://github.com/panr" target="_blank">panr</a></span>
      </div>
  </div>
</footer>






<script type="text/javascript" src="/bundle.min.js"></script>





  
</div>

</body>
</html>
