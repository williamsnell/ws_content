<!DOCTYPE html>
<html lang="en">
<head>
  
    <title>Posts :: willsnell</title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="" />
<meta name="keywords" content="" />

  <meta name="robots" content="noodp" />

<link rel="canonical" href="https://willsnell.com/posts/" />






  
  
  
  
  
  <link rel="stylesheet" href="https://willsnell.com/styles.css">







  <link rel="shortcut icon" href="https://willsnell.com/img/theme-colors/orange.png">
  <link rel="apple-touch-icon" href="https://willsnell.com/img/theme-colors/orange.png">



<meta name="twitter:card" content="summary" />

  
    <meta name="twitter:site" content="" />
  
    <meta name="twitter:creator" content="" />



<meta property="og:locale" content="en" />
<meta property="og:type" content="website" />
<meta property="og:title" content="Posts">
<meta property="og:description" content="" />
<meta property="og:url" content="https://willsnell.com/posts/" />
<meta property="og:site_name" content="willsnell" />

  
    <meta property="og:image" content="https://willsnell.com/img/favicon/orange.png">
  

<meta property="og:image:width" content="1200">
<meta property="og:image:height" content="627">





  <link href="/posts/index.xml" rel="alternate" type="application/rss+xml" title="willsnell" />










  
    
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)']]                  
    }
  };
</script>


  

</head>
<body class="orange">


<div class="container center headings--one-size">

  <header class="header">
  <div class="header__inner">
    <div class="header__logo">
      <a href="/">
  <div class="logo">
    will_snell
  </div>
</a>

    </div>
    
      <ul class="menu menu--mobile">
  <li class="menu__trigger">Menu&nbsp;▾</li>
  <li>
    <ul class="menu__dropdown">
      
        
          <li><a href="/about">About</a></li>
        
      
      
    </ul>
  </li>
</ul>

    
    
  </div>
  
    <nav class="navigation-menu">
  <ul class="navigation-menu__inner menu--desktop">
    
      
        
          <li><a href="/about" >About</a></li>
        
      
      
    
  </ul>
</nav>

  
</header>


  <div class="content">
    
  
  <div class="posts">
    
      <article class="post on-list">
        <h1 class="post-title">
          <a href="https://willsnell.com/posts/trigrams/">Trigrams</a>
        </h1>
        <div class="post-meta"><time class="post-date">2024-12-04</time></div>

        

        
  <img src="/posts/trigrams/cover.png"
    class="post-cover"
    alt="Trigrams"
    title="Cover Image" />


        <div class="post-content">
          
            <blockquote>
<p>This post is converted from a Jupyter Notebook. To view the original interactive version, check out the
<a href="https://colab.research.google.com/drive/161EE2W98h_mpphESWv_mPtbv__MZP8jV?usp=sharing">Colab notebook.</a></p>
</blockquote>
<h1 id="monthly-algorithmic-challenge-november-2024-trigrams">Monthly Algorithmic Challenge (November 2024): Trigrams</h1>
<p>Last week, I worked through the monthly Mechanistic Interpretability challenge from <a href="https://arena3-chapter1-transformer-interp.streamlit.app/Monthly_Algorithmic_Problems">Callum McDougall&rsquo;s ARENA course</a>.</p>
<p><em>(A huge shoutout to Callum and the entire ARENA team for all the work they do!)</em></p>
<p>The challenge was to interpret how a simple neural net - in this case, a 1 layer 1 head transformer (with MLP) - solves a problem.
The problem at hand was to predict the next token in a sequence of random tokens. As the model was trained with cross-entropy
loss, training on a completely random dataset would lead the model to always uniformly predict all tokens in the vocabulary.</p>
          
        </div>

        
          <div>
            <a class="read-more button" href="/posts/trigrams/"> →</a>
          </div>
        
      </article>
    
      <article class="post on-list">
        <h1 class="post-title">
          <a href="https://willsnell.com/posts/entropy/">Entropy and Information Theory</a>
        </h1>
        <div class="post-meta"><time class="post-date">2024-11-18</time></div>

        

        
  <img src="/posts/entropy/adria-berrocal-forcada-o7PxpvonuRQ-unsplash.jpg"
    class="post-cover"
    alt="Entropy and Information Theory"
    title="Cover Image" />


        <div class="post-content">
          
            Learning about Information Theory can be fun! Join me as we explore what entropy and information mean, intuitively.
          
        </div>

        
          <div>
            <a class="read-more button" href="/posts/entropy/"> →</a>
          </div>
        
      </article>
    
      <article class="post on-list">
        <h1 class="post-title">
          <a href="https://willsnell.com/posts/math_composition/">Why Is Measuring Composition So Difficult?</a>
        </h1>
        <div class="post-meta"><time class="post-date">2024-10-12</time><span class="post-author">Will Snell</span></div>

        
          <span class="post-tags">
            
            #<a href="https://willsnell.com/tags/"></a>&nbsp;
            
            #<a href="https://willsnell.com/tags/"></a>&nbsp;
            
          </span>
        

        
  <img src="/posts/math_composition/DSC_0621.jpg"
    class="post-cover"
    alt="Why Is Measuring Composition So Difficult?"
    title="Cover Image" />


        <div class="post-content">
          
            <blockquote>
<p>This post was inspired by, and heavily leans upon, the structure set out in
<a href="https://transformer-circuits.pub/2021/framework/index.html">&ldquo;A Mathematical Framework for Transformer Circuits.&rdquo;</a>
In it, the authors present
a compelling way to decompose the transformer architecture into individual,
more interpretable pieces. <br>
<br>
If you haven&rsquo;t read it yet, I&rsquo;d recommend doing so. Most of what I present
won&rsquo;t make sense without context.</p>
</blockquote>
<h1 id="motivation">Motivation</h1>
<p>A transformer&rsquo;s ability to process long sequences of text is
facilitated by multiple attention layers, which we can decompose into
multiple attention <em>heads</em> per layer.</p>
          
        </div>

        
          <div>
            <a class="read-more button" href="/posts/math_composition/"> →</a>
          </div>
        
      </article>
    
      <article class="post on-list">
        <h1 class="post-title">
          <a href="https://willsnell.com/posts/hyperspace/">Navigating Hyperspace</a>
        </h1>
        <div class="post-meta"><time class="post-date">2024-08-06</time></div>

        

        
  <img src="/posts/hyperspace/hyperspace.png"
    class="post-cover"
    alt="Navigating Hyperspace"
    title="Cover Image" />


        <div class="post-content">
          
            Hyperspaces are unintuitive, strange, and fascinating places. Let&rsquo;s explore them together, interactively.
          
        </div>

        
          <div>
            <a class="read-more button" href="/posts/hyperspace/"> →</a>
          </div>
        
      </article>
    
      <article class="post on-list">
        <h1 class="post-title">
          <a href="https://willsnell.com/posts/gans/">Why does my GAN do that?</a>
        </h1>
        <div class="post-meta"><time class="post-date">2024-07-22</time></div>

        

        
  <img src="/posts/gans/creepy_face_upscaled.png"
    class="post-cover"
    alt="Why does my GAN do that?"
    title="Cover Image" />


        <div class="post-content">
          
            <h2 id="what-are-gans">What are GANs?</h2>
<p>Much ink has already been spilled on the class of machine learning networks called
GANs, or Generative Adversarial Networks, so I will only summarize it here.</p>
<blockquote>
<p>If you&rsquo;re interested in learning more, <a href="https://developers.google.com/machine-learning/gan/gan_structure">this short course is a great resource.</a></p>
</blockquote>
<p>Although replaced in contemporary applications by diffusion models for
tasks like image generation, GANs provide a unique opportunity
to study the interplay of two tightly-coupled systems, each seeking a
different goal.</p>
          
        </div>

        
          <div>
            <a class="read-more button" href="/posts/gans/"> →</a>
          </div>
        
      </article>
    

    <div class="pagination">
  <div class="pagination__buttons">
    
    
  </div>
</div>

  </div>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright">
        <span>© 2024 Powered by <a href="https://gohugo.io">Hugo</a></span>
    
      <span>:: <a href="https://github.com/panr/hugo-theme-terminal" target="_blank">Theme</a> made by <a href="https://github.com/panr" target="_blank">panr</a></span>
      </div>
  </div>
</footer>






<script type="text/javascript" src="/bundle.min.js"></script>





  
</div>

</body>
</html>
