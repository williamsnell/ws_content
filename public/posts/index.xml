<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on willsnell</title>
    <link>http://localhost:1313/posts/</link>
    <description>Recent content in Posts on willsnell</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Thu, 25 Jul 2024 12:19:22 +1200</lastBuildDate><atom:link href="http://localhost:1313/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Navigating Hyperspace</title>
      <link>http://localhost:1313/posts/latent_space/</link>
      <pubDate>Thu, 25 Jul 2024 12:19:22 +1200</pubDate>
      
      <guid>http://localhost:1313/posts/latent_space/</guid>
      <description>Why explore hyperspace? Hyperspace is actually very commonly used in real world applications. A common, but not exclusive use, is in neural networks. The motivating example for this article comes from the sub-field of Generative Adversarial Networks.
The following portraits were generated by Nvidia&amp;rsquo;s StyleGAN:
To dramatically oversimplify, these neural networks take an input vector, do some transformations on it, and produce an image. The input vector is typically random noise, and the vector is often quite long - hundreds or thousands of elements long.</description>
      <content>&lt;h1 id=&#34;why-explore-hyperspace&#34;&gt;Why explore hyperspace?&lt;/h1&gt;
&lt;p&gt;Hyperspace is actually very commonly used in real world applications. A common, but not exclusive
use, is in neural networks. The motivating
example for this article comes from the sub-field of &lt;a href=&#34;https://developers.google.com/machine-learning/gan&#34;&gt;Generative Adversarial Networks&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The following portraits were generated by Nvidia&amp;rsquo;s StyleGAN:&lt;/p&gt;
&lt;hr&gt;
&lt;div style=&#34;display: flex; justify-content: space-around;&#34;&gt;
&lt;img src=&#34;./near_origin_scale/degen_near_origin_with_scalelerp_0.00.jpg&#34;
alt=&#34;A painting of a woman, looking to the left&#34; style=&#34;box-sizing: border-box; width: calc(min(45%, 400px))&#34;/&gt;
&lt;img src=&#34;./near_origin_scale/degen_near_origin_with_scalelerp_1.00.jpg&#34;
alt=&#34;A painting of a man, looking to the right&#34; style=&#34;box-sizing: border-box; width: calc(min(45%, 400px))&#34;/&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;p&gt;To dramatically oversimplify, these neural networks take an input vector,
do some transformations on it, and produce an image. The input vector
is typically random noise, and the vector is often quite long - hundreds or thousands of elements long.&lt;/p&gt;
&lt;p&gt;These vectors make up the &lt;strong&gt;latent space&lt;/strong&gt; of the model. Because these
vectors are long, the latent space is high dimensional.
Manipulating the outputs of these models relies on us
being able to chart paths through their latent space.&lt;/p&gt;
&lt;p&gt;For example, if we want to smoothly blend from the first painting above to the second,
we need a way to traverse from the vector representing one image
to the vector representing the other.&lt;/p&gt;
&lt;h2 id=&#34;the-obvious-answer-is-wrong&#34;&gt;The obvious answer (is wrong)&lt;/h2&gt;
&lt;p&gt;To get from point A to point B, the obvious answer is to go in as straight a
line as possible. The simplest answer here is also the shortest.
Indeed, when working with latent spaces, going in a straight line does generally work,
with varying degrees of success. This is known as linear interpolation (lerp), and
would be written something like this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;lerp&lt;/span&gt;(fraction, start_vec, stop_vec):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; start_vec &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; fraction &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; (stop_vec &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; start_vec)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;script src=&#34;./plotly-2.32.0.min.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;math_lib.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;charts.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;vector_math.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;interp.js&#34;&gt;&lt;/script&gt;
&lt;p&gt;In 3D-space, this looks like:&lt;/p&gt;
&lt;div id=&#34;3d_lerp&#34;&gt;&lt;/div&gt;
&lt;script&gt;
const demo_start = [-0.5012528962436638, -0.9103151253007502, 0.5048315888047492];
const demo_stop = [0.905189016060779, -0.28742159270964684, -0.0913802767988876];
const vec_space_1000 = rand(100, 1000);
let redraw_3d_lerp = get_interpolated_chart(vec_space_1000, &#34;3d_lerp&#34;, lerp, demo_start, demo_stop,
                                          identity_transform);
redraw_3d_lerp(3, 0, identity_transform);
&lt;/script&gt;
&lt;blockquote&gt;
&lt;p&gt;Note: Most of the plots on this page are interactive! Have a play!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;While exploring this topic, I came across a &lt;a href=&#34;https://github.com/soumith/dcgan.torch/issues/14&#34;&gt;befuddling thread&lt;/a&gt;
which suggested that the best path was &lt;strong&gt;not, in fact, a straight line&lt;/strong&gt;. Rather, a function called
&lt;code&gt;slerp&lt;/code&gt;, or &amp;ldquo;spherical linear interpolation&amp;rdquo;, was suggested. This has the rather complicated
functional form:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;slerp&lt;/span&gt;(fraction, start_vec, stop_vec):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        omega &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;arccos(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;clip(np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dot(start_vec&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;linalg&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;norm(start_vec), stop_vec&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;linalg&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;norm(stop_vec)), &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        so &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sin(omega)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# Revert to linear interpolation if the two vectors are pi radians apart&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; so &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; fraction) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; start_vec &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; fraction &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; stop_vec &lt;span style=&#34;color:#75715e&#34;&gt;# L&amp;#39;Hopital&amp;#39;s rule/LERP&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sin((&lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;fraction)&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;omega) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; so &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; start_vec &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sin(fraction&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;omega) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; so &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; stop_vec
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Even more confusingly, when plotted in 3D space, this function gives a path that looks like
this:&lt;/p&gt;
&lt;div id=&#34;3d_slerp&#34;&gt;&lt;/div&gt;
&lt;script&gt;
let redraw_3d_slerp = get_interpolated_chart(vec_space_1000, &#34;3d_slerp&#34;, slerp, demo_start, demo_stop,
                                          identity_transform);
redraw_3d_slerp(3, 0, identity_transform);
&lt;/script&gt;
&lt;p&gt;When thinking about what a &amp;ldquo;good&amp;rdquo; interpolation path might look like, a few
different ideas come to mind. We want it to be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Smooth&lt;/em&gt; - in my experience, jagged, jerky paths do not work well
for blending between two latent vectors.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Relatively short&lt;/em&gt;, since we care about capturing the changes
between two specific points, rather than going sightseeing
to irrelevant destinations&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Well-trodden&lt;/em&gt; - We&amp;rsquo;ve trained our neural net on a limited set of data.
If our interpolation takes us far outside anything the neural net
has ever seen, it&amp;rsquo;s unlikely to perform well.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Looking at slerp, we can see:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It is &lt;em&gt;smooth&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;It doesn&amp;rsquo;t look particularly short. In fact, it&amp;rsquo;s much &lt;em&gt;longer&lt;/em&gt;
than our straight-line path.&lt;/li&gt;
&lt;li&gt;It doesn&amp;rsquo;t seem to stick particularly closely to the data we&amp;rsquo;ve
trained the network on. In fact, it sometimes goes &lt;strong&gt;outside
of our (-1, 1) domain&lt;/strong&gt; entirely!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In 2D and 3D space, linear interpolation simply doesn&amp;rsquo;t have the issues
that slerp does. &lt;strong&gt;Yet, slerp is consistently recommended.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Clearly, something about hyperspace behaves very non-intuitively!&lt;/p&gt;
&lt;p&gt;Strap in&amp;hellip; because it&amp;rsquo;s time to go exploring.&lt;/p&gt;
&lt;h1 id=&#34;windows-into-hyperspace&#34;&gt;Windows into Hyperspace&lt;/h1&gt;
&lt;p&gt;Let&amp;rsquo;s start with the concept of vectors.&lt;/p&gt;
&lt;p&gt;A vector is just a collection of numbers,
arranged in a single column or row, like so:&lt;/p&gt;
\[
\begin{align}
    \vec{v}_{3} = \begin{bmatrix}
        1.9 \\
        4.7 \\
        -3.1 \\
    \end{bmatrix}
\end{align}
\]
&lt;p&gt;An n-dimensional vector is \(n\) items long:&lt;/p&gt;
\[
\begin{align}
    \vec{v}_{n} = \begin{bmatrix}
        x_{1} \\
        \vdots \\
        x_{n}
    \end{bmatrix}
\end{align}
\]
&lt;p&gt;Vectors can be used to represent all sorts of things, but here we&amp;rsquo;re
going to use them to represent &lt;em&gt;cartesian coordinates&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&#34;1-space&#34;&gt;1-space&lt;/h2&gt;
&lt;p&gt;If we had only 1 spatial dimension to play with, we could represent every
possible position with a 1-dimensional vector:&lt;/p&gt;
\[
\begin{align}
    \vec{v}_{1} = \begin{bmatrix}
        x_{1} \\
    \end{bmatrix}
\end{align}
\]
&lt;p&gt;If we were to fill our space with lots of random points, uniformly
distributed from -1 to 1, it would look like this:&lt;/p&gt;
&lt;div id=&#34;1d_space_chart&#34; class=&#34;plotly&#34;&gt;&lt;/div&gt;
&lt;script&gt;
const vec_space = rand(1000, 1);
get_2d_chart(vec_space, &#34;1d_space_chart&#34;, 0, [&#34;&#34;, &#34;&#34;, &#34;&#34;]);
&lt;/script&gt;
&lt;p&gt;Hopefully, this result is pretty unsurprising.&lt;/p&gt;
&lt;h2 id=&#34;2-space&#34;&gt;2-space&lt;/h2&gt;
&lt;p&gt;If we extend our vectors into two dimensions, and perform the same exercise, we&amp;rsquo;ll get
something like this:&lt;/p&gt;
&lt;div id=&#34;2d_space_chart&#34; class=&#34;plotly&#34;&gt;&lt;/div&gt;
&lt;script&gt;
const vec_space_2 = rand(1_000, 2);
get_2d_chart(vec_space_2, &#34;2d_space_chart&#34;, 0, [&#34;&#34;, &#34;&#34;, &#34;&#34;]);
&lt;/script&gt;
&lt;p&gt;For every possible location in this space, we can
define an exact point through something like:&lt;/p&gt;
\[
\begin{align}
    \vec{v}_{2} = \begin{bmatrix}
        -0.85 \\
        0.24 \\
    \end{bmatrix}
\end{align}
\]
&lt;h2 id=&#34;3-space&#34;&gt;3-space&lt;/h2&gt;
&lt;p&gt;Extending up to 3D is quite straightforward, where we
now have 3-long vectors like this:&lt;/p&gt;
&lt;div id=&#34;3_vec&#34;&gt;
\[
\begin{align}
    \vec{v}_{3} = \begin{bmatrix}
        0.26 \\
        -0.88 \\
        -0.9 \\
    \end{bmatrix}
\end{align}
\]
&lt;/div&gt;
&lt;p&gt;Let&amp;rsquo;s again scatter some points uniformly between -1 and 1,
this time in 3 dimensions:&lt;/p&gt;
&lt;div id=&#34;3d_space_chart&#34; style=&#34;width: 100%;&#34;&gt;&lt;/div&gt;
&lt;script&gt;
const vec_space_3 = rand(10_000, 3);
get_3d_chart(vec_space_3, &#34;3d_space_chart&#34;, 0, [&#34;&#34;, &#34;&#34;, &#34;&#34;]);
&lt;/script&gt;
&lt;p&gt;We&amp;rsquo;re very used to looking at 3D space through these kinds of visualizations, where
our brain can reconstruct a series of 2D images into a 3D representation.&lt;/p&gt;
&lt;h2 id=&#34;flattening-space&#34;&gt;Flattening Space&lt;/h2&gt;
&lt;p&gt;What if we wanted to look at &lt;strong&gt;this 4D&lt;/strong&gt; vector
inside its vector space:&lt;/p&gt;
\[
\begin{align}
    \vec{v}_{4} = \begin{bmatrix}
        0.93  \\
        -0.43 \\
        0.67  \\
        0.12  \\
    \end{bmatrix}
\end{align}
\]
&lt;p&gt;We could &lt;em&gt;try&lt;/em&gt; using time as an extra dimension,
but we&amp;rsquo;ve already run out of spatial dimensions.&lt;/p&gt;
&lt;p&gt;Of course, we want to go far beyond a mere &lt;em&gt;four&lt;/em&gt; dimensions.
Even if we used
How would we visualize something like this?&lt;/p&gt;
\[
\begin{align}
    \vec{v}_{1000} = \begin{bmatrix}
        x_{1}      \\
        x_{2}      \\
        \vdots     \\
        x_{1000}   \\
    \end{bmatrix}
\end{align}
\]
&lt;h2 id=&#34;projecting&#34;&gt;Projecting&lt;/h2&gt;
&lt;p&gt;To glimpse higher dimensions, we&amp;rsquo;re
necessarily going to need to make compromises.
With &lt;em&gt;up to&lt;/em&gt; 3 dimensions to play with, any given
viewport will need to choose what information to show and what to hide.&lt;/p&gt;
&lt;p&gt;A natural way to project higher dimensions is to just take
the first \(n\) dimensions we can display, and ignore the rest.&lt;/p&gt;
&lt;p&gt;We can visualize what this looks like by creating a vector space
in three dimensions, and visualizing it with two.&lt;/p&gt;
&lt;p&gt;If we want to display the vector:&lt;/p&gt;
\[
\begin{align}
    \vec{v} = \begin{bmatrix}
        0.21   \\
        -0.85  \\
        -0.32  \\
    \end{bmatrix}
\end{align}
\]
&lt;p&gt;We can display the first 2 elements, i.e.:&lt;/p&gt;
\[
\begin{align}
    \vec{c}_2 = \begin{bmatrix}
        0.21  \\
        -0.85 \\
    \end{bmatrix}
\end{align}
\]
&lt;p&gt;Where \(\vec{c}_2\) represents a &lt;strong&gt;cartesian projection&lt;/strong&gt;
down to 2 dimensions.&lt;/p&gt;
&lt;p&gt;We can write this as an equation:&lt;/p&gt;
\[
    \vec{v}_3 \mapsto \vec{c}_2
\]
&lt;p&gt;Where the arrow \(\mapsto\) means
&amp;ldquo;maps to&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Visualized, it looks like so:&lt;/p&gt;
&lt;div id=&#34;3d_into_2d&#34; style=&#34;width: 100%;&#34;&gt;&lt;/div&gt;
&lt;script&gt;
get_2d_3d_chart(vec_space_3, &#34;3d_into_2d&#34;);
&lt;/script&gt;
&lt;p&gt;We can pick any 2 elements to display, of course.
Representing our 3-space in 2 dimensions could
be done equally validly by picking two different
elements, such as the last element \(x_{3}\)
and the second element \(x_{2}\):&lt;/p&gt;
\[
\begin{align}
    \vec{v} = \begin{bmatrix}
        -0.32 \\
        -0.85 \\
    \end{bmatrix}
\end{align}
\]
&lt;p&gt;What does our 2D projection tell us about the 3D space?
Well, we effectively get the same view as if we rotated
our 3D view until we were just looking at one face.&lt;/p&gt;
&lt;p&gt;If we&amp;rsquo;re plotting, say, \(x_{1}\) and \(x_{2}\),
we get a perfect understanding of how our points are
distributed in those two dimensions.&lt;/p&gt;
&lt;p&gt;Should we want to know
what portion of points have \(x_{1}\) &amp;gt; 0
and \(x_{2}\) &amp;lt; 0, we can
look at the 2D chart and easily see the answer is
~25%.&lt;/p&gt;
&lt;p&gt;However, we get absolutely no information about the
rest of our vector. It wouldn&amp;rsquo;t matter if we were
plotting a vector of length 3 or a vector of length
3000 - from this viewpoint, they all look the same.&lt;/p&gt;
&lt;h2 id=&#34;different-projections&#34;&gt;Different Projections&lt;/h2&gt;
&lt;p&gt;So far, we&amp;rsquo;ve been exploring space with &lt;em&gt;cartesian&lt;/em&gt; coordinates.&lt;/p&gt;
&lt;p&gt;Without completely justifying it, I&amp;rsquo;m going to introduce
a completely different coordinate system - &lt;a href=&#34;https://en.wikipedia.org/wiki/Spherical_coordinate_system&#34;&gt;spherical coordinates&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Most people are used to &amp;ldquo;cartesian&amp;rdquo; coordinates. In the following
image, it seems natural to define the position of the red cross based
on two distances, which we typically call x and y.
&lt;img src=&#34;xy.svg&#34;&gt;&lt;/p&gt;
&lt;p&gt;We could represent this point as a vector:&lt;/p&gt;
\[
\begin{align}
    \vec{v}_2 = \begin{bmatrix}
        x \\
        y \\
    \end{bmatrix}
\end{align}
\]
&lt;p&gt;In higher dimensions, we can add more directions, provided they are
perpendicular to all the other directions. Hence, for 3d, we might
use (x, y, z).&lt;/p&gt;
&lt;p&gt;In a spherical coordinate system, however, a point in space is defined
not by \(n\) orthogonal coordinates (e.g. x, y, and z), but rather
as a &lt;em&gt;radial distance&lt;/em&gt; \(r\), and then a series of angles.&lt;/p&gt;
&lt;p&gt;To fully describe any point in 2D-space, we need two coordinates.
Since we already have one (the distance from the origin \(r\)),
we need one more. Hence, a 2D spherical coordinate system would have
one angle, \(\theta_1\).&lt;/p&gt;
&lt;img src=&#34;radial.svg&#34;&gt;
&lt;p&gt;We can also represent this point as a vector:&lt;/p&gt;
\[
\begin{align}
    \vec{s}_2 = \begin{bmatrix}
        r          \\
        \theta_{1} \\
    \end{bmatrix}
\end{align}
\]
&lt;p&gt;Notice that &lt;strong&gt;both \(\vec{v}_2\) and \(\vec{s}_2\)&lt;/strong&gt; refer to
the exact same point in space. The actual numbers inside the vectors,
and the coordinate &lt;strong&gt;system&lt;/strong&gt; used are very different, but the point
in space is the same.&lt;/p&gt;
&lt;h3 id=&#34;adding-dimensions&#34;&gt;Adding Dimensions&lt;/h3&gt;
&lt;p&gt;In 3-space, we need a third coordinate. For cartesian coordinates, we add z
to our existing x and y. For spherical coordinates, we add
another angle \(\theta_2\).&lt;/p&gt;
&lt;p&gt;These two vectors represent the same position:&lt;/p&gt;
\[
\begin{align}
    \vec{v} = \begin{bmatrix}
        0.54  \\
        -0.87 \\
        0.26  \\
    \end{bmatrix}_{[x,y,z]}
    = \vec{s} = \begin{bmatrix}
        1.06  \\
        -1.02 \\
        1.32  \\
    \end{bmatrix}_{[r, \theta_1, \theta_2]}
\end{align}
\]
&lt;h3 id=&#34;why-bother-with-spherical-coordinates&#34;&gt;Why bother with spherical coordinates?&lt;/h3&gt;
&lt;p&gt;How does this help us? After all, you still
need an n-length vector to represent a point in n-space.&lt;/p&gt;
&lt;p&gt;What&amp;rsquo;s interesting, however, is when you start looking at
higher dimensions. Since the length \(r\) takes into account
the entire vector, plotting the first 2 or 3 elements in the
spherical vector gives us a different view on higher dimensions.&lt;/p&gt;
&lt;p&gt;Importantly, &lt;strong&gt;we always keep the magnitude of the full vector&lt;/strong&gt;
when using spherical coordinates.&lt;/p&gt;
&lt;p&gt;We then get to select 1 angle (for a 2D plot) or 2 angles (for
a 3D plot). These angles represent the relative positioning
between &lt;strong&gt;some, but not all&lt;/strong&gt; elements of the vector.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://localhost:1313/posts/latent_space/#projecting&#34;&gt;Earlier, we projected higher-dimensional space&lt;/a&gt; into 2D and 3D
cartesian plots. We got to pick 2 elements from our larger vector, and had to
throw away the rest.&lt;/p&gt;
&lt;p&gt;We have to do a similar thing in spherical coordinates. However, we &lt;em&gt;always&lt;/em&gt;
keep the magnitude. This means that we&amp;rsquo;re left with the ability to pick
one angle (for a 2d plot) or 2 angles (for a 3d plot) from our larger
vector.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Below, you can increase the dimensionality of the space being
visualized.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Before you do&lt;/strong&gt;, make a guess about what you think
will happen as the number of dimensions increases.&lt;/p&gt;
&lt;p&gt;Remember, we&amp;rsquo;re keeping the &lt;em&gt;vector magnitude&lt;/em&gt;, but
can only keep one angle (for the 2D plot) or 2
angles (for the 3D plot).&lt;/p&gt;
&lt;p&gt;How many dimensions do you think we can plot before
the spherical projection will start to look different
to the cartesian projection?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div id=&#34;spherical&#34; style=&#34;width: 100%;&#34;&gt;&lt;/div&gt;
&lt;div id=&#34;spherical_vec&#34;&gt;&lt;/div&gt;
&lt;div id=&#34;tooltip-1space&#34; style=&#34;display: none;&#34;&gt;
&lt;blockquote&gt;
&lt;p&gt;// 1-space&lt;/p&gt;
&lt;p&gt;1-space is boring as ever&amp;hellip;&lt;/p&gt;
&lt;p&gt;Jump to the next space with the &amp;ldquo;Dimensions (+)&amp;rdquo; button.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;tooltip-2space&#34; style=&#34;display: none;&#34;&gt;
&lt;blockquote&gt;
&lt;p&gt;// 2-space&lt;/p&gt;
&lt;p&gt;In 2-space, both the 2D and the 3D plot display the same
thing. This is also the exact same view we would get if we were
using cartesian coordinates. Because any 2-length vector losslessly
describes this space, we can freely switch between them without issue.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;tooltip-3space&#34; style=&#34;display: none;&#34;&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: understanding the 2D and 3D plots at for a 3D space
is critical to understanding the rest of this article. Take the time
to try and wrap your head around the link between these two plots.&lt;/p&gt;
&lt;p&gt;// 3-space&lt;/p&gt;
&lt;p&gt;Our &lt;strong&gt;3D plot&lt;/strong&gt; still holds enough
dimensionality to perfectly represent our vector, and so our view is
identical to the cartesian plot we had earlier. That is,
our mapping \(\vec{v} \mapsto \vec{s}\) is lossless.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;2D plot&lt;/strong&gt;, however, is different. We&amp;rsquo;re fundamentally
losing some information when projecting from \(\vec{v}_3\) to \(\vec{s}_2\).
Notably, even though our points are randomly
distributed between -1 and 1, we are starting to see points shift outside
of that range.&lt;/p&gt;
&lt;p&gt;Remember that the distance from the origin (0, 0) in our 2D plot now
represents the absolute distance from the origin in &lt;strong&gt;n-space&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Questions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Looking at the 3D view of the cube, which points do you think
have a distance to
the origin (a &lt;em&gt;vector magnitude&lt;/em&gt;) greater than 1?&lt;/p&gt;
&lt;p&gt;Interestingly, a hole has started to appear in the centre of the plot.
Why do you think this is?&lt;/p&gt;
&lt;p&gt;What might you expect to see happen as we continue to increase the dimensionality
of our space?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;tooltip-4space&#34; style=&#34;display: none;&#34;&gt;
&lt;blockquote&gt;
&lt;p&gt;// 4-space&lt;/p&gt;
&lt;p&gt;This is the first space that cannot be fully represented by the
spatial dimensions we have at hand. If you&amp;rsquo;ve been watching the 2D
plot over the last few dimensionalities, you should be able to guess
what&amp;rsquo;s coming for our 3-space plot.&lt;/p&gt;
&lt;p&gt;This is also the first dimensionality where we get multiple 3D and 2D
plots to hop between. By pressing the &amp;ldquo;Elements (-)&amp;rdquo; or &amp;ldquo;Elements (+)&amp;rdquo;
buttons, we can move through the vector, choosing different elements
to act as the &amp;ldquo;direction&amp;rdquo; component of our spherical projection.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;tooltip-5space&#34; style=&#34;display: none;&#34;&gt;
&lt;blockquote&gt;
&lt;p&gt;// 5-space and beyond&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ll leave you be as you explore the next few dimensions.&lt;/p&gt;
&lt;p&gt;Have a play around, and try and build an intuition for
what these charts are telling you about the spaces.&lt;/p&gt;
&lt;p&gt;Remember, the &lt;strong&gt;Dimensions&lt;/strong&gt; buttons change the dimensionality
of the underlying vector space, and the &lt;strong&gt;Elements&lt;/strong&gt; buttons
change which elements of \(\vec{v}\) we&amp;rsquo;re using to calculate
\(\theta_1\) and \(\theta_2\).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;p&gt;You can also change the noise distribution to:
&lt;span&gt;&lt;button id=&#34;noise_gaussian&#34; style=&#34;display: block;&#34; onclick=&#34;
// change which button is displayed
document.getElementById(&#39;noise_uniform&#39;).style.display = &#39;block&#39;;
document.getElementById(&#39;noise_gaussian&#39;).style.display = &#39;none&#39;;
// redraw the last chart
redraw_proj(randn(10000, 1000));
&#34;&gt;gaussian&lt;/button&gt;&lt;/span&gt;
&lt;span&gt;&lt;button id=&#34;noise_uniform&#34; style=&#34;display: none;&#34; onclick=&#34;
// change which button is displayed
document.getElementById(&#39;noise_gaussian&#39;).style.display = &#39;block&#39;;
document.getElementById(&#39;noise_uniform&#39;).style.display = &#39;none&#39;;
// redraw the last chart
redraw_proj(vec_space_1000);
&#34;&gt;uniform&lt;/button&gt;&lt;/span&gt;&lt;/p&gt;
&lt;script&gt;
let dims_with_text = [1, 2, 3, 4, 5];
function redraw_proj(vector_space) {
    document.getElementById(&#39;spherical&#39;).innerHTML = &#39;&#39;;
    document.getElementById(&#39;spherical_vec&#39;).innerHTML = &#39;&#39;;

    let redraw_spherical = get_projected_chart(vector_space, &#39;spherical&#39;, [&#34;&#34;, &#34;&#34;, &#34;&#34;], vecs_to_spherical);
    let callback = (dimensions, slice_offset) =&gt; {
        for (let dim of dims_with_text) {
            if (dim == dimensions) {
                document.getElementById(`tooltip-${dimensions}space`).style.display = &#34;block&#34;;
            } else if (dimensions &gt; dims_with_text[dims_with_text.length - 1]) {
                document.getElementById(`tooltip-${dims_with_text[dims_with_text.length - 1]}space`).style.display = &#34;block&#34;;
            } else {
                document.getElementById(`tooltip-${dim}space`).style.display = &#34;none&#34;;
            }
        }
        redraw_spherical(dimensions, slice_offset);
    }
    let widget = get_vector_widget(vector_space[0], &#39;spherical_vec&#39;, callback, 1);
}
redraw_proj(vec_space_1000);
&lt;/script&gt;
&lt;h2 id=&#34;whats-going-on&#34;&gt;What&amp;rsquo;s going on?&lt;/h2&gt;
&lt;p&gt;Our projection has shown us an unintuitive, but true, fact about
hyperspace - as the dimensionality increases, our points converge
to a hyperspherical shell. The radius of this shell scales with
the square root of our initial distribution&amp;rsquo;s variance, \(\sqrt{\sigma^2} = \sigma\),
and with the square root of our dimensionality, \(\sqrt{n}\).&lt;/p&gt;
&lt;p&gt;The exact formula for the radius varies depending on the type of
noise used (results for a &lt;a href=&#34;https://stats.stackexchange.com/questions/317095/expectation-of-square-root-of-sum-of-independent-squared-uniform-random-variable/317475#317475&#34;&gt;uniform distribution&lt;/a&gt;
and this &lt;a href=&#34;https://www.johndcook.com/blog/2011/09/01/multivariate-normal-shell/&#34;&gt;great post with results for normal distributions&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;For both uniform and normal distributions, the hyperspherical
shell has a relatively constant thickness as the dimensionality
increases, leading to an increasingly shell-like distribution of points.&lt;/p&gt;
&lt;h3 id=&#34;what-does-this-mean&#34;&gt;What does this mean?&lt;/h3&gt;
&lt;p&gt;In lower dimension spaces (2D, 3D, etc.) the radius of our hypershell
is of the same order as the variance of the distribution. This means
that, in general, there isn&amp;rsquo;t much of a &amp;ldquo;hole&amp;rdquo; at the origin. However,
even in 3D (using our 2D plot), we start to see a gap open up near the
origin.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s consider two different interpretations of how to interpret the
existence of a hyperspherical shell.&lt;/p&gt;
&lt;h3 id=&#34;geometric-interpretation&#34;&gt;Geometric Interpretation&lt;/h3&gt;
&lt;p&gt;As explained in &lt;a href=&#34;https://www.johndcook.com/blog/2011/09/01/multivariate-normal-shell/&#34;&gt;John D. Cook&amp;rsquo;s post&lt;/a&gt;,
volume &lt;strong&gt;grows faster&lt;/strong&gt; in higher dimensions.
For our uniform distribution, our probability density is constant between its bounds of (-1, 1),
and so we can pretty much ignore it.&lt;/p&gt;
&lt;p&gt;Volume, however, is proportional to \(r^n\), where \(r\) is the distance
from the origin and \(n\) is the dimensionality of our space.
If \(n = 1000\) dimensions, the difference between a sphere of radius
0.999 and radius 1.000 is&lt;/p&gt;
\[
1.000^{1000} - 0.99^{1000} \approx 0.9999
\]
&lt;p&gt;In other words, &amp;gt;99% of all of our volume is contained in an outer shell, with
thickness of 1% the radius of our space.
The reason this collosal growth of volume with radius is not intuitive, is because
in 3D, the same calculation would give around 3% of the volume in the outermost
shell of our sphere:&lt;/p&gt;
\[
1.000^3 - 0.99^3 \approx 0.03 
\]
&lt;p&gt;Hence, even though our probability density function is constant in space,
when we go to higher dimensions, &lt;strong&gt;the amount of space near the origin is astronomically
low, and the amount at the outer perimeter is astronomically high.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Because so much space is so far out, our points will inevitably &amp;ldquo;cluster&amp;rdquo;
there.&lt;/p&gt;
&lt;h3 id=&#34;statistical-interpretation&#34;&gt;Statistical Interpretation&lt;/h3&gt;
&lt;p&gt;We can also think about this result statistically.&lt;/p&gt;
&lt;p&gt;All the elements in our vectors are independent and identically distributed.
The more elements we have, the more we will expect to see strong statistical
trends in the overall properties of our vector, even while individual
elements remain random.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s imagine we&amp;rsquo;re rolling a fair die, with sides labelled 0, 1, 2, 3, 4, and 5.
The expected value of our roll is 2.5, but we wouldn&amp;rsquo;t be surprised with a 0
or a 5.&lt;/p&gt;
&lt;p&gt;If we now roll 2 dice, make a graph, and plot our first roll on the x axis
and our second on the y axis, we again get a fairly even distribution.&lt;/p&gt;
&lt;p&gt;However, if we instead added the total of our two dice together, we
would be looking at a score between 0 and 10, with 5 being our expected value.
Already, our sum is starting to cluster, with 5 much more likely than
either 0 or 10.&lt;/p&gt;
&lt;p&gt;The more dice we roll:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The bigger we expect our total score to be, and&lt;/li&gt;
&lt;li&gt;The less and less likely we are to have a sum near 0 (or near the absolute
highest possible score of \(5 \times n\) rolls.)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The same process, roughly, is going on with the magnitude of our vectors.
Instead of just summing our rolls, we&amp;rsquo;re squaring each roll, summing the
squares, and then taking the square root. These functions warp and
compress space a bit, but our intuition should generally still hold.&lt;/p&gt;
&lt;p&gt;We should intuitively expect that the more dice we roll,&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The bigger our square-root sum of squares is, and&lt;/li&gt;
&lt;li&gt;The less and less likely we are to have a point near the origin
(or in the corners of our hypercube.)&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;tracing-lines-through-hyperspace&#34;&gt;Tracing Lines Through Hyperspace&lt;/h1&gt;
&lt;p&gt;Hopefully, you now have a solid grip on the spherical projections
we&amp;rsquo;ll be using from this point onwards. Remember, the distance
from a point to the origin in each plot represents the vector magnitude of
the &lt;em&gt;full vector&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Under this lens, what does linear interpolation (our &lt;code&gt;lerp&lt;/code&gt; function from earlier)
look like?&lt;/p&gt;
&lt;div id=&#34;spherical_lerp&#34;&gt;&lt;/div&gt;
&lt;div id=&#34;lerp_vec&#34;&gt;&lt;/div&gt;
&lt;script&gt;
let redraw_chart = get_interpolated_chart(vec_space_1000, &#34;spherical_lerp&#34;, lerp, vec_space_1000[0], vec_space_1000[1],
                                          vecs_to_spherical);
let widget2 = get_vector_widget(vec_space_1000[0], &#34;lerp_vec&#34;, redraw_chart, 1);
&lt;/script&gt;
&lt;p&gt;At low dimensions, lerp behaves exactly how we expect it to. But by the time we reach
around 20 dimensions, there&amp;rsquo;s a clear problem. Our linear path is well outside
the bounds of all the points in our vector space.&lt;/p&gt;
&lt;p&gt;As we increase the dimensionality of our space, the problem gets worse. At 1000 dimensions,
lerp spends almost the entirety of its path completely outside of the hyperspherical shell
that makes up the points in our vector space.&lt;/p&gt;
&lt;p&gt;In a machine learning context, this would mean that the interpolation is feeding in data
well outside the bounds of anything the model has been trained on.&lt;/p&gt;
&lt;h2 id=&#34;why-does-lerp-behave-like-this-in-higher-dimensional-spaces&#34;&gt;Why Does Lerp Behave Like This in Higher Dimensional Spaces?&lt;/h2&gt;
&lt;h1 id=&#34;slerp&#34;&gt;Slerp&lt;/h1&gt;
&lt;div id=&#34;spherical_slerp&#34;&gt;&lt;/div&gt;
&lt;div id=&#34;slerp_vec&#34;&gt;&lt;/div&gt;
&lt;script&gt;
let redraw_slerp = get_interpolated_chart(vec_space_1000, &#34;spherical_slerp&#34;, slerp, vec_space_1000[0], vec_space_1000[1],
                                          vecs_to_spherical);
let widget3 = get_vector_widget(vec_space_1000[0], &#34;slerp_vec&#34;, redraw_slerp, 1);
&lt;/script&gt;
&lt;h1 id=&#34;slerp_2&#34;&gt;Slerp_2&lt;/h1&gt;
&lt;div id=&#34;spherical_slerp2&#34;&gt;&lt;/div&gt;
&lt;div id=&#34;slerp_vec2&#34;&gt;&lt;/div&gt;
&lt;script&gt;
let redraw_slerp2 = get_interpolated_chart(vec_space_1000, &#34;spherical_slerp2&#34;, slerp2, vec_space_1000[0], vec_space_1000[1],
                                          vecs_to_spherical);
let widget3_2 = get_vector_widget(vec_space_1000[0], &#34;slerp_vec2&#34;, redraw_slerp2, 1);
&lt;/script&gt;
&lt;h1 id=&#34;using-it-in-practice&#34;&gt;Using it in practice&lt;/h1&gt;
&lt;h2 id=&#34;degenerate-case-1-almost-passing-through-the-origin&#34;&gt;Degenerate Case 1: (Almost) passing through the origin&lt;/h2&gt;
&lt;h2 id=&#34;degenerate-case-2-almost-passing-through-the-origin-with-elevation-changes&#34;&gt;Degenerate Case 2: (Almost) passing through the origin with elevation changes&lt;/h2&gt;
&lt;h2 id=&#34;comparing-the-lerps-with-real-vectors&#34;&gt;Comparing the &amp;rsquo;lerps with real vectors&lt;/h2&gt;
&lt;div id=&#34;through_origin&#34;&gt;&lt;/div&gt;
&lt;script&gt;
let stylegan_space = randn(10000, 512);
fetch(&#34;vecs.json&#34;)
    .then(response =&gt; response.json())
    .then(jvecs =&gt; {
    start_through_origin = jvecs.z;
    stop_through_origin = add(mult(jvecs.z, -1), 1e-4);
    let redraw = get_multi_interp_chart(stylegan_space, &#34;through_origin&#34;, lerp, start_through_origin, stop_through_origin, 
                                        &#34;./near_origin&#34;, [0.00, 0.14, 0.29, 0.43, 0.57, 0.71, 0.86, 1.00],
                                        vecs_to_spherical);
    redraw(512, 0);
    });
&lt;/script&gt;
&lt;div id=&#34;through_origin_with_scale&#34;&gt;&lt;/div&gt;
&lt;script&gt;
fetch(&#34;vecs.json&#34;)
    .then(response =&gt; response.json())
    .then(jvecs =&gt; {
    start_scale = mult(jvecs.z, 0.9999);
    stop_scale = add(mult(jvecs.z, -1.0001), 1e-4);
    let redraw_scale = get_interpolated_chart(stylegan_space, &#34;through_origin_with_scale&#34;, slerp2, start_scale, stop_scale,
                                        vecs_to_spherical);
    redraw_scale(512, 0);
    });
&lt;/script&gt;
# Interpretations
&lt;p&gt;Geometric:&lt;/p&gt;
&lt;p&gt;Great Post! &lt;a href=&#34;https://www.johndcook.com/blog/2011/09/01/multivariate-normal-shell/&#34;&gt;https://www.johndcook.com/blog/2011/09/01/multivariate-normal-shell/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Think about what we saw as we increased the number of dimensions of our space,
keeping the number of points constant. If you&amp;rsquo;d like, jump back to &lt;a href=&#34;http://localhost:1313/posts/latent_space/#why_bother_with_spherical_coordinates&#34;&gt;the interactive
plots&lt;/a&gt; and hop between 1, 2, and 3 dimensions.&lt;/p&gt;
&lt;p&gt;We start with a line, very densely packed. Then we have a square, which
is already less dense. Both dimensions are still, of course, uniformly
distributed between -1 and 1, but already our distance between any two
points has increased.&lt;/p&gt;
&lt;p&gt;Next, we have a cube, and already, the points are so far apart that the
10,000 or so points plotted don&amp;rsquo;t look particularly dense. Even though
we can&amp;rsquo;t see it, when we next jump to a hypercube,
we can expect our space to decrease in density once again.&lt;/p&gt;
&lt;p&gt;The extra dimensions&lt;/p&gt;
&lt;p&gt;Statistical:&lt;/p&gt;
&lt;p&gt;The law of large numbers says that as our number of dimensions increases, our
vectors will begin to strongly exhibit properties of the underlying distribution.&lt;/p&gt;
&lt;p&gt;In particular, the variance of our distribution tells us the expected distance
away from our mean, which in this case is the origin.&lt;/p&gt;
&lt;h1 id=&#34;whats-next&#34;&gt;What&amp;rsquo;s next?&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;other measures (density / mean distance to neighbours)&lt;/li&gt;
&lt;li&gt;skewness&lt;/li&gt;
&lt;li&gt;exploring distributions (slerp keeps the vector magnitude right, but skews the variance of
the intermediate vectors.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Sources:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/NVlabs/stylegan2-ada-pytorch/tree/main&#34;&gt;https://github.com/NVlabs/stylegan2-ada-pytorch/tree/main&lt;/a&gt;
and
&lt;a href=&#34;https://arxiv.org/pdf/2006.06676&#34;&gt;https://arxiv.org/pdf/2006.06676&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/soumith/dcgan.torch/issues/14&#34;&gt;https://github.com/soumith/dcgan.torch/issues/14&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Shoemake
&lt;a href=&#34;https://dl.acm.org/doi/pdf/10.1145/325334.325242&#34;&gt;https://dl.acm.org/doi/pdf/10.1145/325334.325242&lt;/a&gt;&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Why does my GAN do that?</title>
      <link>http://localhost:1313/posts/gans/</link>
      <pubDate>Mon, 22 Jul 2024 09:25:17 +1200</pubDate>
      
      <guid>http://localhost:1313/posts/gans/</guid>
      <description>What are GANs? Much ink has already been spilled on the class of machine learning networks called GANs, or Generative Adversarial Networks, so I will only summarize it here.
If you&amp;rsquo;re interested in learning more, this short course is a great resource.
Although replaced in contemporary applications by diffusion models for tasks like image generation, GANs provide a unique opportunity to study the interplay of two tightly-coupled systems, each seeking a different goal.</description>
      <content>&lt;h2 id=&#34;what-are-gans&#34;&gt;What are GANs?&lt;/h2&gt;
&lt;p&gt;Much ink has already been spilled on the class of machine learning networks called
GANs, or Generative Adversarial Networks, so I will only summarize it here.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If you&amp;rsquo;re interested in learning more, &lt;a href=&#34;https://developers.google.com/machine-learning/gan/gan_structure&#34;&gt;this short course is a great resource.&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Although replaced in contemporary applications by diffusion models for
tasks like image generation, GANs provide a unique opportunity
to study the interplay of two tightly-coupled systems, each seeking a
different goal.&lt;/p&gt;
&lt;p&gt;GANs consist of two distinct networks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a generator, which tries to generate new, convincingly realistic content, and&lt;/li&gt;
&lt;li&gt;a discriminator, which tries to tell real content from fake.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Generally, during training, the generator is allowed to back-propagate gradients
all the way through both the discriminator (as it assesses the generated images)
and the generator itself. Conversely, the discriminator has no special knowledge
of the inner workings of the generator.
Hence, if a generator&amp;rsquo;s output fails to fool the discriminator, the
generator gets immediate feedback about what parts of the image tipped the discriminator off
to the fakery. But if a discriminator is repeatedly hoodwinked, it can only look inward to
understand how to improve.&lt;/p&gt;
&lt;p&gt;Although GANs can technically generate any type of content, this post will focus on the generation
of images.&lt;/p&gt;
&lt;h2 id=&#34;visualizing-dcgans&#34;&gt;Visualizing DCGANs&lt;/h2&gt;
&lt;p&gt;I particularly like using visualisation to get a deeper grip on a complex system
I am working with, and Deep Convolutional GANs (&lt;a href=&#34;https://arxiv.org/pdf/1511.06434&#34;&gt;as introduced in Radford, Metz &amp;amp; Chintala&lt;/a&gt;)
offered the perfect opportunity for some interesting visualisations.&lt;/p&gt;

  &lt;figure class=&#34;left&#34; &gt;
    &lt;img src=&#34;dcgan_gen_architecture.png&#34;   /&gt;
    
      &lt;figcaption class=&#34;center&#34; &gt;The DCGAN architecture (Radford, Metz &amp; Chintala), specifically the generator, with a 100-tall vector being processed through convolutional blocks that gradually decrease in feature depth and increase in resolution, until being projected to a 64 x 64 pixel RGB image.&lt;/figcaption&gt;
    
  &lt;/figure&gt;


&lt;p&gt;DCGANs use ordinary 2-dimensional convolutions in the discriminator to repeatedly downsample the image, building up
more and more internal features as the spatial resolution decreases.
In this way, they a variant of a typical convolutional image classifier.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The architecture presented in the DCGAN paper omits residual/skip-connections, à la &lt;a href=&#34;https://arxiv.org/pdf/1512.03385&#34;&gt;ResNet&lt;/a&gt;.
However, the two networks were introduced at similar times, and so the DCGAN&amp;rsquo;s authors probably didn&amp;rsquo;t shun them on purpose.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The generator does effectively the same thing, but in reverse, taking a large (in this case 100-long)
vector of random noise, and up-projecting it via transposed convolutions to gradually increase the
image resolution while decreasing the depth of the feature dimension.&lt;/p&gt;
&lt;p&gt;Because this network processes images, the very
first layer of the discriminator and the very last layer of the generator
should produce kernels that take in/out image data directly.&lt;/p&gt;
&lt;p&gt;For a (4x4 kernel) convolutional
layer that takes in an RGB image and produces a 128-feature output, this comes out to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;128 individual kernels (1 for each feature in the output layer),&lt;/li&gt;
&lt;li&gt;of depth 3 (i.e. each kernel has different weights for how much it activates
on a red, green, or blue pixel)&lt;/li&gt;
&lt;li&gt;of size 4 x 4&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Because each kernel has a depth of 3, we can visualize its weights using RGB images. In essence,
we get 128 x (4 high x 4 wide) images, where a bright red pixel indicates a part of the kernel
that activates strongly on red pixels, but not on green or blue, and a black pixel indicates no
activation for any input colour.&lt;/p&gt;
&lt;p&gt;We can use this process in reverse to visualize the generator&amp;rsquo;s &lt;strong&gt;output&lt;/strong&gt; kernels, where a 128-feature
space is projected up into an RGB image by transpose convolutions (effectively, but not quite, a convolution in reverse.)
The generator&amp;rsquo;s kernels are the different brushes with which the network paints the output image. Just like an artist might
need brushes of different sizes and shapes to effectively draw broad strokes and details, so the network might be expected
to need to specialize into different output kernels.&lt;/p&gt;
&lt;p&gt;At least, that&amp;rsquo;s my intuition for what would be expected to appear. In image classifiers (read: the discriminator),
the input kernels are often visualized in this way.&lt;/p&gt;
&lt;p&gt;For example, this image shows (centre) the kernels of a ResNet as visualized by &lt;a href=&#34;https://www.researchgate.net/publication/321192231&#34;&gt;Jiang, et. al&lt;/a&gt;:
&lt;img alt=&#34;Resnet in Jiang, et. al&#34; src=&#34;https://www.researchgate.net/publication/321192231/figure/fig4/AS:963217320841220@1606660313749/Visualization-of-first-layer-convolution-kernels-and-feature-maps-for-the-CS-ResCNN.gif&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;hypothesis-1&#34;&gt;Hypothesis 1&lt;/h2&gt;
&lt;p&gt;Before running the experiment, I expected that the discriminator would
develop clearly identifiable features in its kernels, such as
alternating black/white lines in various orientations (useful for edge-detection,) potentially filters
of one main colour, etc.&lt;/p&gt;
&lt;p&gt;I suspected the generator would be similar, but was less confident in this. After all, the kernel
visualizations I had seen, to date, were all of classifiers/discriminators, which serve a different
purpose to the generator I was going to train.&lt;/p&gt;
&lt;p&gt;What I saw, however, was much more surprising.&lt;/p&gt;
&lt;h2 id=&#34;exploring-my-dcgan&#34;&gt;Exploring my DCGAN&lt;/h2&gt;
&lt;p&gt;I started with a DCGAN with feature layers of depth [128, 256, 512], kernel size (4 x 4), and which took in an 64x64 RGB image.
The generator had an input vector of size 100, and was trained on normally distributed random noise.&lt;/p&gt;
&lt;p&gt;Training on the &lt;a href=&#34;https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html&#34;&gt;CelebA&lt;/a&gt; dataset, I used a batch-size of 8, using PyTorch
with an Adam optimizer using default settings for both the generator and the discriminator.&lt;/p&gt;
&lt;p&gt;The results from this first run are presented below.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note: I&amp;rsquo;ve used a non-linear
time-step in the video since the model should change less and less as it gets further
and further through its training.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;video controls preload=&#34;auto&#34; width=&#34;500px&#34;  autoplay loop playsinline class=&#34;html-video&#34;&gt;
    &lt;source src=&#34;http://localhost:1313/posts/gans/gan_full.mp4&#34; type=&#34;video/mp4&#34;&gt;
  &lt;span&gt;Your browser doesn&#39;t support embedded videos, but don&#39;t worry, you can &lt;a href=&#34;http://localhost:1313/posts/gans/gan_full.mp4&#34;&gt;download it&lt;/a&gt; and watch it with your favorite video player!&lt;/span&gt;
&lt;/video&gt;
&lt;p&gt;&lt;em&gt;Training a [128, 256, 512] feature DCGAN on CelebA for 3 epochs.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;These results were very surprising -
essentially the exact opposite of what I expected to see.
While I had suspected the generator kernels might be inscrutable, many of
them instead appeared to have an identifiable function.&lt;/p&gt;
&lt;p&gt;On the other hand, the discriminator kernels appeared to not change at all
from their initialisation. Although I suspected an issue with the code,
further investigation showed the discriminator&amp;rsquo;s kernels were, in fact, changing
throughout the run - just not enough to be discernible.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The values in the input kernels to the discriminator do change, slightly.
In the visualization, the layer becomes noticeably less saturated from start to finish, even though
the absolute values of the layer weights do not change substantially. Since I normalize
each frame by the max and min pixel values, this suggests to me that a few pixels are
&amp;lsquo;going hot&amp;rsquo;, but that
they are not doing so in a recognisable pattern.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;hypothesis-2&#34;&gt;Hypothesis 2&lt;/h2&gt;
&lt;p&gt;My hypothesis, based on the above results, is as follows:&lt;/p&gt;
&lt;p&gt;Assumptions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The capacity of a network to learn information during training should be correlated to
the size of the network.&lt;/li&gt;
&lt;li&gt;A layer retaining its initial values, or something very close to them, suggests that
the exact distribution of these layers is unimportant to the network.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Based on these (admittedly unproven) assumptions, I would make a few predictions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If a layer is unimportant to the network, the network is probably over-sized for the task it is being
trained on&lt;/li&gt;
&lt;li&gt;Therefore, a smaller network (with its initial layer decreased in size) should be able to do equally well
on the task.&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Doing equally well on the task&amp;rdquo;, in this context, is not strictly what the discriminator is evaluated on (i.e. its ability
to distinguish real from fake images), but rather is the quality of images produced after the GAN is fully trained.&lt;/li&gt;
&lt;li&gt;A well sized discriminator will show specialization in its input kernels.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Boiling this down, I suspected that decreasing the number of input kernels of the discriminator (leaving the generator untouched)
would have little to no impact on the quality of images produced after training. I also suspected the combined network would
work equally well up to the point where the discriminator&amp;rsquo;s input kernels showed strong specialization (i.e. something similar to
the generator&amp;rsquo;s output kernels.)&lt;/p&gt;
&lt;h2 id=&#34;testing-hypothesis-2&#34;&gt;Testing Hypothesis 2&lt;/h2&gt;
&lt;p&gt;The obvious test was to decrease the number of input kernels for the discriminator
(i.e. the first-layer feature depth) from 128 to 64, and see what happens.&lt;/p&gt;
&lt;p&gt;Even though GANs are notoriously unstable during training, this change didn&amp;rsquo;t cause the network to diverge.
This suggests to me that the discriminator might have an
easier job to do than the generator, and so the smaller network was not immediately defeated
by its adversary.&lt;/p&gt;
&lt;video controls preload=&#34;auto&#34; width=&#34;500px&#34;  autoplay loop playsinline class=&#34;html-video&#34;&gt;
    &lt;source src=&#34;http://localhost:1313/posts/gans/first_layer_64.mp4&#34; type=&#34;video/mp4&#34;&gt;
  &lt;span&gt;Your browser doesn&#39;t support embedded videos, but don&#39;t worry, you can &lt;a href=&#34;http://localhost:1313/posts/gans/first_layer_64.mp4&#34;&gt;download it&lt;/a&gt; and watch it with your favorite video player!&lt;/span&gt;
&lt;/video&gt;
&lt;p&gt;There are some interesting takeaways from these results:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The output kernels from the generator look very similar to those from the previous
model. This suggests these kernels represent features useful to the generator,
regardless of the exact distribution of features in the hidden layers.&lt;/li&gt;
&lt;li&gt;The discriminator&amp;rsquo;s first layer still looks remarkably random. Despite being half as big,
the fully trained layer still looks like a desaturated version of the starting layer. At
least one recognisable feature seems to have appeared, though, in the 3rd column, 1st row.&lt;/li&gt;
&lt;li&gt;The image quality produced by the fully trained generator looks noticeably worse than for
the previous network. Although broad facial features are still present, the images qualitatively
look desaturated and hazy.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These findings suggest that although the input kernel&amp;rsquo;s exact weights may not be important, it is important
for the discriminator to have access to lots of them. Even if the feature representation inside the model
is ultimately fed by a series of random convolutions, having that feature depth seems to give the discriminator
more tools with which to sniff out fraudulent images. Consequently, a smaller number of input kernels lets the
generator get away with worse images.&lt;/p&gt;
&lt;h2 id=&#34;putting-the-hypothesis-properly-to-bed&#34;&gt;Putting the Hypothesis Properly to Bed&lt;/h2&gt;
&lt;p&gt;To check that these findings hold, I ran 3 more models with smaller and smaller input kernels. In particular,
the number of input features to the discriminator were decreased to 32, 16, and then 8. Mainly, I wanted to see
if the discriminator would eventually need to start specializing these kernel weights, even if it didn&amp;rsquo;t want to.&lt;/p&gt;
&lt;h3 id=&#34;32-input-kernels&#34;&gt;32 Input Kernels&lt;/h3&gt;
&lt;video controls preload=&#34;auto&#34; width=&#34;500px&#34;  autoplay loop playsinline class=&#34;html-video&#34;&gt;
    &lt;source src=&#34;http://localhost:1313/posts/gans/first_layer_32.mp4&#34; type=&#34;video/mp4&#34;&gt;
  &lt;span&gt;Your browser doesn&#39;t support embedded videos, but don&#39;t worry, you can &lt;a href=&#34;http://localhost:1313/posts/gans/first_layer_32.mp4&#34;&gt;download it&lt;/a&gt; and watch it with your favorite video player!&lt;/span&gt;
&lt;/video&gt;
&lt;p&gt;The 32 input-kernel network follows the same trend as the 64 input-kernel model: increasingly hazy, desaturated images.&lt;/p&gt;
&lt;p&gt;Two interesting notes, though:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Even as the image quality is degraded, the quality of the facial features looks pretty similar.&lt;/li&gt;
&lt;li&gt;More egregious structured artifacts are beginning to appear. In the lower left of this image, for example,
you can see a clear checker-boarding pattern in the background. Such patterns are commonly produced by
this network.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt=&#34;Blocky, repeating artifacts visible in the lower-left-corner&#34; src=&#34;http://localhost:1313/posts/gans/artefacts.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Together, these findings suggest to me that deeper layers of the discriminator
are still learning a good representation of facial features. However, the constrained
first layer has hindered the ability of the discriminator to notice and penalize noise and
overall &amp;ldquo;image-quality&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;I imagine each decrease in input kernels as the discriminator looking through an increasingly blurry
or perhaps tinted lens at the image, able to make out broad strokes but missing areas of high spatial frequency,
and seeing an increasingly desaturated version of the world.
The analogy isn&amp;rsquo;t quite right, since the spatial dimensions the network receives are unchanged, but it&amp;rsquo;s a start.&lt;/p&gt;
&lt;h3 id=&#34;the-edge-of-functionality---8-input-kernels&#34;&gt;The Edge of Functionality - 8 Input Kernels&lt;/h3&gt;
&lt;p&gt;During training, the 16-kernel GAN diverged, meaning the last model left to explore has 8 input kernels.&lt;/p&gt;
&lt;video controls preload=&#34;auto&#34; width=&#34;500px&#34;  autoplay loop playsinline class=&#34;html-video&#34;&gt;
    &lt;source src=&#34;http://localhost:1313/posts/gans/first_layer_8.mp4&#34; type=&#34;video/mp4&#34;&gt;
  &lt;span&gt;Your browser doesn&#39;t support embedded videos, but don&#39;t worry, you can &lt;a href=&#34;http://localhost:1313/posts/gans/first_layer_8.mp4&#34;&gt;download it&lt;/a&gt; and watch it with your favorite video player!&lt;/span&gt;
&lt;/video&gt;
&lt;p&gt;Finally, we see clear changes from the input kernels at the start to those at the end. However, this is
not enough to save the network, and the fully trained GAN produces images with little colour saturation,
degraded facial features, little separation between background and foreground, and most obviously, serious
checker-boarding artifacts across every part of the image.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s perhaps remarkable that such a constrained discriminator could cause the generator to build up anything
resembling a human face, but the final results are indisputably bad, and much worse than previous models.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Even though the input kernels to the discriminator in a DCGAN did not show much specialization during training,
reducing the number of input kernels available showed obvious reductions in image quality produced by the generator
after training. Further, the input kernels to the discriminator, no matter how few they were,
never showed the kind of specialization apparent in the final layers of the generator.&lt;/p&gt;
&lt;h3 id=&#34;other-wild-geese-to-chase&#34;&gt;Other Wild Geese to Chase&lt;/h3&gt;
&lt;p&gt;Even though the DCGAN&amp;rsquo;s Discriminator and Generator are largely symmetric, there are some key differences
that might explain the apparent lack of specialization in the discriminator&amp;rsquo;s first layer.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Activation functions: the generator uses Tanh between the transpose convolution and the final generated image,
while the discriminator a) uses LeakyReLU and b) places the activation function after the convolution. I haven&amp;rsquo;t
visualized the effect of these activation functions (since doing so would require an input to be run through
the network, and the result would depend on said input.) It&amp;rsquo;s quite possible the answer to this mystery lies
here.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The discriminator is not actually trained in the same way as typical classifiers. Although the architecture is
similar, the output of this discriminator is a single value (real or fake.)
It&amp;rsquo;s possible random noise is perfectly sufficient for
this task, whereas specialization only becomes necessary for classifiers that need to distinguish between
multiple different classes of image.&lt;/p&gt;
&lt;p&gt;Testing this would be as simple* as changing datasets (e.g. to &lt;a href=&#34;https://github.com/zalandoresearch/fashion-mnist&#34;&gt;fashion-MNIST&lt;/a&gt;
or &lt;a href=&#34;https://www.cs.toronto.edu/~kriz/cifar.html&#34;&gt;CIFAR-10&lt;/a&gt;), and changing the discriminator to
choose from 11 categories (the 10 from CIFAR-10, plus 1 extra denoting &amp;ldquo;fake&amp;rdquo;).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;*No guarantees it will be this simple&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The flow of gradients through the network for a GAN is asymmetric: the generator gets access
to the entire state of the discriminator for backprop, while the discriminator only gets its
own network&amp;rsquo;s response to stimulus to learn from. Why this mismatch would lead to such a
difference in layer specialization is unknown, to me at least. Perhaps patterned specialization
in the discriminator&amp;rsquo;s layer closest to the generator&amp;rsquo;s own layers would provide an easy
attack vector for the generator to learn and defeat the discriminator?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Is the first layer actually random?&lt;/em&gt; The lack of human-identifiable structure in the kernels
doesn&amp;rsquo;t strictly mean the discriminator isn&amp;rsquo;t specializing them. Freezing this layer and comparing
performance to the original network could tell whether the discriminator is, in fact, getting
value from this layer after all.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</content>
    </item>
    
  </channel>
</rss>
